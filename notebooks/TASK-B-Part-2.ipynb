{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074e24f9-a286-43cd-965b-1c41044ee99b",
   "metadata": {},
   "source": [
    "# Task B: Part-2 : Int8 Quantization\n",
    "\n",
    "This notebook builds and trains a recurrent neural network (LSTM) to classify spoken digits (0â€“9) from audio recordings.\n",
    "\n",
    "- Dataset: [Free Spoken Digit Dataset (FSDD)](https://github.com/Jakobovski/free-spoken-digit-dataset)\n",
    "- Framework: PyTorch\n",
    "- Architecture: RNN with LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac121864-dea8-4206-b0d1-1acbdc0002dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root (parent of current folder) to Python path\n",
    "project_root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907bf1a-f498-4c4b-941a-1ec9859a6dbc",
   "metadata": {},
   "source": [
    "## Load Model Configuration from YAML\n",
    "\n",
    "To make the training pipeline configurable and modular, we store model parameters like number of LSTM layers, hidden size, and learning rate etc in a YAML file. This structure enables quick adaptation to related tasks B, and C.\n",
    "\n",
    "This section loads the model configuration using a custom utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "fc007882-2f6c-4e2e-9a19-18eb3a3dd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f5efd2eb-0415-43c9-ba38-492a135899d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "utils.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "51de20a8-e68c-40e3-b17b-f2cc330329ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "\n",
    "model_config_path = os.path.join(project_root_dir, 'config', 'model_config.yaml')\n",
    "model_config = utils.read_yaml_file(model_config_path)\n",
    "# print(json.dumps(model_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db000b-c161-4faf-b555-80a64696bed4",
   "metadata": {},
   "source": [
    "## Load and Split Dataset for Training and Evaluation\n",
    "\n",
    "In this section, we load the recordings data from disk, generate data-label pairs, and split them into training and test sets according to the `test_size` defined in the YAML file.\n",
    "\n",
    "Using `test_size` and `seed` from the YAML config ensures that experiments are reproducible and easily tunable for other tasks by simply updating the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fd4c30c2-fdc9-4d1f-8491-8948e8a844c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = model_config['dataset']['path']\n",
    "test_data_size = model_config['data_splitting']['test_size']\n",
    "seed = model_config['experiment']['seed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3f2e678e-b078-4b6d-8788-4807fc2912a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label_pairs, calibration_samples = utils.prepare_data_label_pairs(data_path, calibration_samples_per_class=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "39326a52-6342-4b71-a59c-c687ac5a0005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(calibration_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d5b6e981-6360-4c48-bf87-4d722523a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data_label_pairs, test_size=test_data_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8a06f-d118-462d-987e-c5ee7363933c",
   "metadata": {},
   "source": [
    "## Transform Raw Data into PyTorch Dataset Objects\n",
    "\n",
    "The `AudioFeaturesDataset` class converts raw data-label pairs into PyTorch-compatible datasets that provide easy access to samples and labels.\n",
    "\n",
    "AudioFeaturesDataset is a custom dataset class that:\n",
    "\n",
    "- Loads audio recordings of spoken digits along with their labels.\n",
    "- Optionally cleans the audio by filtering out noise.\n",
    "- Extracts MFCC features (a common speech feature).\n",
    "- Pads or trims these features to a fixed length so all inputs have the same shape.\n",
    "- Works with PyTorch to provide samples one-by-one when training or testing a model.\n",
    "- It helps prepare your audio data in the right format for training neural networks efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d7a181bb-fce0-4dc5-aba6-b0cc522854d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessor import AudioFeaturesDataset\n",
    "\n",
    "train_dataset = AudioFeaturesDataset(train_data)\n",
    "test_dataset = AudioFeaturesDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "4121f284-fd0e-49aa-a47b-ebfde0f1e4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2400\n",
      "Test size: 600\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682500e-8660-481b-a3a8-823d4e824486",
   "metadata": {},
   "source": [
    "## Create DataLoaders for Batch Processing\n",
    "\n",
    "Using PyTorch DataLoaders, we enable efficient loading, batching, and shuffling of data during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "a2957cdf-bee7-4877-87c8-c4eeb186b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a30ecd48-25db-4e42-92e7-173fd8894e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = model_config['model']['input_dim']\n",
    "hidden_dim = model_config['model']['hidden_dim']\n",
    "num_layers = model_config['model']['num_layers']\n",
    "output_dim = model_config['model']['output_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b6a3b93e-82ab-4347-99ef-5be89844a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29cc61-758b-4310-bc8a-8d2a0513449f",
   "metadata": {},
   "source": [
    "## Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "cbcce1b9-ded8-4bcb-a57a-3fb4e7975aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.model as model\n",
    "\n",
    "hidden_dim = 24\n",
    "quant_model = model.LSTMClassifier(input_dim=input_dim,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       num_layers=num_layers,\n",
    "                       output_dim=output_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "9bc6c82b-ae04-4c45-9114-45b20908b919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_constraint_model_load_path = os.path.join(project_root_dir, 'outputs', 'models', 'task-b-part-1_weights.pth')\n",
    "quant_model.load_state_dict(torch.load(memory_constraint_model_load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "c557645b-3ae1-472b-90ce-d632e4ac8476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (lstm): LSTM(13, 24, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=24, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "74102fd8-de55-41a7-99d8-ff589f26527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.quantize as quantize\n",
    "static_quant_model = quantize.StaticQuantizableModel(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "b0aa6bea-00c5-444c-a8f6-a832e3cb0922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/ao/quantization/observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/ao/nn/quantizable/modules/rnn.py:462: UserWarning: dropout option for quantizable LSTM is ignored. If you are training, please, use nn.LSTM version followed by `prepare` step.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "static_quant_model.qconfig = torch.quantization.get_default_qconfig('x86')  # For edge devices\n",
    "# torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "from torch.ao.quantization import MinMaxObserver, PerChannelMinMaxObserver, QConfig, MovingAverageMinMaxObserver, MovingAveragePerChannelMinMaxObserver\n",
    "qconfig = QConfig(activation=MovingAveragePerChannelMinMaxObserver.with_args(qscheme=torch.per_tensor_affine, dtype=torch.quint8),\n",
    "       weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n",
    "\n",
    "static_quant_model_prepared = torch.quantization.prepare(static_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "285c2385-70b7-4c0d-945f-7d597de40cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration set size : 500\n"
     ]
    }
   ],
   "source": [
    "calibration_sampleset = AudioFeaturesDataset(calibration_samples)\n",
    "calibration_loader = torch.utils.data.DataLoader(calibration_sampleset, batch_size=32, shuffle=True)\n",
    "print(f\"Calibration set size : {len(calibration_sampleset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "5440b3c5-07d9-4e5b-a71f-9a94ea7c21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_quant_model_prepared_cpu = static_quant_model_prepared.to('cpu')\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "quantize.calibrate(static_quant_model_prepared, calibration_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "702c7a4e-ca23-4526-801b-86f40fac6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_quantized_model_converted = torch.quantization.convert(static_quant_model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "d0e93b23-1526-4397-b534-9365d9f9cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate import ModelEvaluator\n",
    "print(device)\n",
    "quant_test_instance = ModelEvaluator(\n",
    "    static_quantized_model_converted, \n",
    "    test_loader,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "81f25534-4412-4767-9d1a-a5fa96700202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/ao/nn/quantizable/modules/rnn.py:543: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:167.)\n",
      "  cx_tensor = torch.stack(cx_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy on test data: 85.83%\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.8272    0.9306    0.8758        72\n",
      "           1     0.9219    0.8551    0.8872        69\n",
      "           2     0.8511    0.7018    0.7692        57\n",
      "           3     0.7179    1.0000    0.8358        56\n",
      "           4     1.0000    0.9492    0.9739        59\n",
      "           5     0.8393    0.7460    0.7899        63\n",
      "           6     0.9600    0.8571    0.9057        56\n",
      "           7     0.8070    0.8364    0.8214        55\n",
      "           8     0.9792    0.8246    0.8952        57\n",
      "           9     0.7778    0.8750    0.8235        56\n",
      "\n",
      "    accuracy                         0.8583       600\n",
      "   macro avg     0.8681    0.8576    0.8578       600\n",
      "weighted avg     0.8688    0.8583    0.8587       600\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      "[[67  0  2  2  0  0  0  0  0  1]\n",
      " [ 1 59  0  0  0  2  0  6  0  1]\n",
      " [12  0 40  4  0  0  0  1  0  0]\n",
      " [ 0  0  0 56  0  0  0  0  0  0]\n",
      " [ 0  3  0  0 56  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 47  0  4  0 12]\n",
      " [ 0  0  0  7  0  0 48  0  1  0]\n",
      " [ 1  2  1  1  0  4  0 46  0  0]\n",
      " [ 0  0  0  8  0  0  2  0 47  0]\n",
      " [ 0  0  4  0  0  3  0  0  0 49]]\n"
     ]
    }
   ],
   "source": [
    "quant_test_instance.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "a4c0e4bd-d360-41a8-ab5d-e861caed9ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model.lstm.layers.0.layer_fw.cell.igates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 13])\n",
      "---\n",
      "\n",
      "model.lstm.layers.0.layer_fw.cell.hgates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 24])\n",
      "---\n",
      "\n",
      "model.lstm.layers.1.layer_fw.cell.igates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 24])\n",
      "---\n",
      "\n",
      "model.lstm.layers.1.layer_fw.cell.hgates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 24])\n",
      "---\n",
      "\n",
      "model.fc is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([10, 24])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for name, module in static_quantized_model_converted.named_modules():\n",
    "    if isinstance(module, torch.nn.quantized.Linear):\n",
    "        print(f\"\\n{name} is a quantized Linear layer\")\n",
    "        weight = module.weight()\n",
    "        print(\"Weight type:\", type(weight))\n",
    "        print(\"Weight dtype:\", weight.dtype)\n",
    "        print(\"Weight shape:\", weight.shape)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "d257145a-26bd-4dba-a432-846542b292c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated FP32 size: 32.81 KB\n",
      "Estimated INT8 size: 8.20 KB\n",
      "Compression ratio: 4.00x\n"
     ]
    }
   ],
   "source": [
    "total_fp32_bytes = 0\n",
    "total_int8_bytes = 0\n",
    "\n",
    "for name, module in static_quantized_model_converted.named_modules():\n",
    "    if isinstance(module, torch.nn.quantized.Linear):\n",
    "        weight = module.weight()\n",
    "        if weight.is_quantized:\n",
    "            num_elements = weight.numel()\n",
    "            total_fp32_bytes += num_elements * 4  # FP32\n",
    "            total_int8_bytes += num_elements * 1  # INT8\n",
    "\n",
    "print(f\"Estimated FP32 size: {total_fp32_bytes / 1024:.2f} KB\")\n",
    "print(f\"Estimated INT8 size: {total_int8_bytes / 1024:.2f} KB\")\n",
    "print(f\"Compression ratio: {total_fp32_bytes / total_int8_bytes:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "f3550aff-1b76-4659-8280-9702e58a477c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer Name                               | Num Parameters | Size (INT8) | Fits in 36KB?\n",
      "---------------------------------------------------------------------------\n",
      "model.lstm.layers.0.layer_fw.cell.igates |          1248 | 1.219 KB     | âœ…\n",
      "model.lstm.layers.0.layer_fw.cell.hgates |          2304 | 2.250 KB     | âœ…\n",
      "model.lstm.layers.1.layer_fw.cell.igates |          2304 | 2.250 KB     | âœ…\n",
      "model.lstm.layers.1.layer_fw.cell.hgates |          2304 | 2.250 KB     | âœ…\n",
      "model.fc                                 |           240 | 0.234 KB     | âœ…\n",
      "\n",
      "ðŸ“Š Total Model Summary\n",
      "Total number of parameters:      8400\n",
      "Estimated total size (INT8):     8.203 KB\n",
      "Memory per parameter (INT8):     1 byte\n"
     ]
    }
   ],
   "source": [
    "def print_full_layer_analysis(model):\n",
    "    print(\"Layer Name\".ljust(40) + \" | Num Parameters | Size (INT8) | Fits in 36KB?\")\n",
    "    print(\"-\" * 75)\n",
    "\n",
    "    total_params = 0\n",
    "    total_kb = 0\n",
    "\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.quantized.Linear):\n",
    "            weight = module.weight()\n",
    "            if weight.is_quantized:\n",
    "                num_params = weight.numel()\n",
    "                kb = (num_params * 1) / 1024  # int8 â†’ 1 byte per param\n",
    "                total_params += num_params\n",
    "                total_kb += kb\n",
    "                fits = kb <= 36.0\n",
    "                print(f\"{name.ljust(40)} | {str(num_params).rjust(13)} | {kb:.3f} KB     | {'âœ…' if fits else 'âŒ'}\")\n",
    "\n",
    "    print(\"\\nðŸ“Š Total Model Summary\")\n",
    "    print(f\"Total number of parameters:      {total_params}\")\n",
    "    print(f\"Estimated total size (INT8):     {total_kb:.3f} KB\")\n",
    "    print(f\"Memory per parameter (INT8):     1 byte\")\n",
    "\n",
    "print_full_layer_analysis(static_quantized_model_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "ae61efa8-e9d3-479c-aee9-4f1d00ef2a65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "model.lstm.layers.0.layer_fw.cell.igates â†’ 1248 params\n",
      "model.lstm.layers.0.layer_fw.cell.hgates â†’ 2304 params\n",
      "model.lstm.layers.1.layer_fw.cell.igates â†’ 2304 params\n",
      "model.lstm.layers.1.layer_fw.cell.hgates â†’ 2304 params\n",
      "model.fc â†’ 240 params\n",
      "\n",
      "âœ… Total quantized parameters: 8400\n"
     ]
    }
   ],
   "source": [
    "def count_quantized_weights(model):\n",
    "    total_params = 0\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, torch.nn.quantized.Linear):\n",
    "            weight = module.weight()\n",
    "            if weight.is_quantized:\n",
    "                num_params = weight.numel()\n",
    "                print(f\"{name} â†’ {num_params} params\")\n",
    "                total_params += num_params\n",
    "    print(f\"\\nâœ… Total quantized parameters: {total_params}\")\n",
    "\n",
    "count_quantized_weights(static_quantized_model_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "418b8710-1ae6-469e-8282-f0acbd8326d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_quantized_model_converted.eval()\n",
    "static_quant_model_save_path = os.path.join(project_root_dir, 'outputs', 'models', 'task-b-part-2_weights_no_jit.pth')\n",
    "torch.save(static_quantized_model_converted.state_dict(), static_quant_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "d8ecd6db-6138-4599-a4f7-5301314b9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_quantized_model_converted.eval()\n",
    "\n",
    "# static_quant_model_save_path = os.path.join(project_root_dir, 'outputs', 'models', 'task-b-part-2_weights_jit.pth')\n",
    "\n",
    "# script_model = torch.jit.script(static_quantized_model_converted)\n",
    "# torch.jit.save(script_model, static_quant_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "2b84f90b-eaa3-4fff-92e2-0fe8c7f21c46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StaticQuantizableModel(\n",
      "  (quant): Quantize(scale=tensor([2.2913]), zero_point=tensor([87]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      "  (model): LSTMClassifier(\n",
      "    (lstm): QuantizedLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): _LSTMLayer(\n",
      "          (layer_fw): _LSTMSingleLayer(\n",
      "            (cell): QuantizableLSTMCell(\n",
      "              (igates): QuantizedLinear(in_features=13, out_features=96, scale=0.82073974609375, zero_point=67, qscheme=torch.per_channel_affine)\n",
      "              (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.04487304016947746, zero_point=55, qscheme=torch.per_channel_affine)\n",
      "              (gates): QFunctional(\n",
      "                scale=0.7883639335632324, zero_point=64\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (input_gate): Sigmoid()\n",
      "              (forget_gate): Sigmoid()\n",
      "              (cell_gate): Tanh()\n",
      "              (output_gate): Sigmoid()\n",
      "              (fgate_cx): QFunctional(\n",
      "                scale=0.4761994779109955, zero_point=62\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (igate_cgate): QFunctional(\n",
      "                scale=0.015740342438220978, zero_point=64\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (fgate_cx_igate_cgate): QFunctional(\n",
      "                scale=0.49226832389831543, zero_point=62\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (ogate_cy): QFunctional(\n",
      "                scale=0.015732653439044952, zero_point=64\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): _LSTMLayer(\n",
      "          (layer_fw): _LSTMSingleLayer(\n",
      "            (cell): QuantizableLSTMCell(\n",
      "              (igates): QuantizedLinear(in_features=24, out_features=96, scale=0.052332811057567596, zero_point=33, qscheme=torch.per_channel_affine)\n",
      "              (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.047158267349004745, zero_point=49, qscheme=torch.per_channel_affine)\n",
      "              (gates): QFunctional(\n",
      "                scale=0.08937833458185196, zero_point=35\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (input_gate): Sigmoid()\n",
      "              (forget_gate): Sigmoid()\n",
      "              (cell_gate): Tanh()\n",
      "              (output_gate): Sigmoid()\n",
      "              (fgate_cx): QFunctional(\n",
      "                scale=0.23494866490364075, zero_point=66\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (igate_cgate): QFunctional(\n",
      "                scale=0.014107674360275269, zero_point=63\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (fgate_cx_igate_cgate): QFunctional(\n",
      "                scale=0.24108541011810303, zero_point=67\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (ogate_cy): QFunctional(\n",
      "                scale=0.01573582924902439, zero_point=64\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): QuantizedDropout(p=0.3, inplace=False)\n",
      "    (fc): QuantizedLinear(in_features=24, out_features=10, scale=0.10456912219524384, zero_point=55, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import copy\n",
    "\n",
    "copied_model = copy.deepcopy(static_quantized_model_converted)\n",
    "\n",
    "print(copied_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e5fd672-5dd0-4f31-acfe-282759721b0d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "7f7a9aac-c0bd-4625-a735-68c9a306fd67",
   "metadata": {},
   "source": [
    "## Task C: Power of 2 Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "5713b725-27ee-40ee-b444-50f8c609fb52",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import math\n",
    "\n",
    "def round_scale_to_power_of_two(scale):\n",
    "    if scale <= 0:\n",
    "        return scale\n",
    "    return 2 ** round(math.log2(scale))\n",
    "\n",
    "def update_quantized_weights_and_show_scales(module: nn.Module):\n",
    "    for name, submodule in module.named_children():\n",
    "        if isinstance(submodule, torch.nn.quantized.Linear):\n",
    "            old_wt = submodule.weight()\n",
    "            qscheme = old_wt.qscheme()\n",
    "\n",
    "            if qscheme == torch.per_tensor_affine:\n",
    "                old_scale = old_wt.q_scale()\n",
    "                old_zp = old_wt.q_zero_point()\n",
    "                float_wt = old_wt.dequantize()\n",
    "                new_scale = round_scale_to_power_of_two(old_scale)\n",
    "                new_qweight = torch.quantize_per_tensor(\n",
    "                    float_wt, scale=new_scale, zero_point=old_zp, dtype=torch.qint8\n",
    "                )\n",
    "                submodule.set_weight_bias(new_qweight, submodule.bias())\n",
    "                print(f\"[Per-tensor] {name}:\")\n",
    "                print(f\"  Old scale: {old_scale:.8f}\")\n",
    "                print(f\"  New scale: {new_scale:.8f}\")\n",
    "                print(f\"  Zero point: {old_zp}\")\n",
    "\n",
    "            elif qscheme == torch.per_channel_affine:\n",
    "                old_scales = old_wt.q_per_channel_scales()\n",
    "                old_zps = old_wt.q_per_channel_zero_points()\n",
    "                axis = old_wt.q_per_channel_axis()\n",
    "                float_wt = old_wt.dequantize()\n",
    "                new_scales = torch.tensor(\n",
    "                    [round_scale_to_power_of_two(s.item()) for s in old_scales],\n",
    "                    dtype=old_scales.dtype,\n",
    "                    device=old_scales.device\n",
    "                )\n",
    "                new_qweight = torch.quantize_per_channel(\n",
    "                    float_wt, new_scales, old_zps, axis=axis, dtype=torch.qint8\n",
    "                )\n",
    "                submodule.set_weight_bias(new_qweight, submodule.bias())\n",
    "                print(f\"[Per-channel] {name}:\")\n",
    "                print(f\"  Old scales: {old_scales.tolist()}\")\n",
    "                print(f\"  New scales: {new_scales.tolist()}\")\n",
    "                print(f\"  Zero points: {old_zps.tolist()}\")\n",
    "\n",
    "            else:\n",
    "                print(f\"[Skipped] {name}: Unsupported qscheme {qscheme}\")\n",
    "\n",
    "        else:\n",
    "            update_quantized_weights_and_show_scales(submodule)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0a093e8a-7cb5-4437-b988-2d1d9b3b79a2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Per-channel] igates:\n",
      "  Old scales: [0.0014680278254672885, 0.0020057877991348505, 0.0019790276419371367, 0.0018683441448956728, 0.002030798699706793, 0.002097557531669736, 0.0016376471612602472, 0.0017878131475299597, 0.002021649619564414, 0.0020765841472893953, 0.0016087722033262253, 0.001678281114436686, 0.0015331507893279195, 0.0019961087964475155, 0.0014595008688047528, 0.0021228939294815063, 0.0019447437953203917, 0.002361265243962407, 0.0015113336266949773, 0.0015943407779559493, 0.0021128575317561626, 0.0018155161524191499, 0.0019345361506566405, 0.0015810549957677722, 0.002372996648773551, 0.0020744006615132093, 0.0020200232975184917, 0.0026737854350358248, 0.002018067752942443, 0.002467712387442589, 0.0017417334020137787, 0.0023375835735350847, 0.0015620372723788023, 0.0016133295139297843, 0.0017817617626860738, 0.0017840875079855323, 0.001950323348864913, 0.00205528037622571, 0.0019604831468313932, 0.0017279349267482758, 0.001883684890344739, 0.001897631329484284, 0.002375006675720215, 0.002434829715639353, 0.0018237981712445617, 0.0018195516895502806, 0.0021796117071062326, 0.0018255853792652488, 0.00203363923355937, 0.002230878686532378, 0.0020253381226211786, 0.0017391110304743052, 0.00186349474824965, 0.001880164840258658, 0.0023672394454479218, 0.0018158007878810167, 0.0017805114621296525, 0.0016362890601158142, 0.0016248221509158611, 0.0023234470281749964, 0.002675885334610939, 0.001948026125319302, 0.0023255739361047745, 0.001760074170306325, 0.0019493824802339077, 0.0015360282268375158, 0.0023606172762811184, 0.001612348947674036, 0.0017982714343816042, 0.0018210670677945018, 0.0022219752427190542, 0.0016041058115661144, 0.0017813495360314846, 0.0024939707946032286, 0.0016591078601777554, 0.0021583442576229572, 0.001662647700868547, 0.0021907805930823088, 0.002023500856012106, 0.0024476952385157347, 0.001606726204045117, 0.0016673015197739005, 0.001984605100005865, 0.001944026560522616, 0.0017607255140319467, 0.0031537082977592945, 0.0019879986066371202, 0.0023427815176546574, 0.002228612545877695, 0.0020566554740071297, 0.0015198617475107312, 0.0022409067023545504, 0.0018107104115188122, 0.0015468377387151122, 0.002201421419158578, 0.0023030166048556566]\n",
      "  New scales: [0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125]\n",
      "  Zero points: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[Per-channel] hgates:\n",
      "  Old scales: [0.0031388290226459503, 0.002873200224712491, 0.0025606337003409863, 0.0027817783411592245, 0.002589656738564372, 0.00271820486523211, 0.002003563567996025, 0.002100827172398567, 0.002335368422791362, 0.0019224559655413032, 0.0019043595530092716, 0.002106413245201111, 0.0025523032527416945, 0.0017890101298689842, 0.0032924883998930454, 0.0024688486009836197, 0.002615926321595907, 0.0022614242043346167, 0.002732452703639865, 0.0025516131427139044, 0.002354028169065714, 0.002699395176023245, 0.003142175730317831, 0.002264629118144512, 0.0026810248382389545, 0.002290577394887805, 0.0025333743542432785, 0.0025359722785651684, 0.003690125420689583, 0.0022456985898315907, 0.0024544682819396257, 0.0024793162010610104, 0.002248343313112855, 0.002599825384095311, 0.0031443126499652863, 0.0026679516304284334, 0.0022454068530350924, 0.002663952764123678, 0.0022588756401091814, 0.0029022309463471174, 0.0023217478301376104, 0.0023571751080453396, 0.00345402373932302, 0.0023733628913760185, 0.002647048095241189, 0.0023731347173452377, 0.0033624216448515654, 0.004151183646172285, 0.0020318857859820127, 0.002385107334703207, 0.0027675556484609842, 0.0024203979410231113, 0.002163949888199568, 0.003068135119974613, 0.0022134145256131887, 0.002040015533566475, 0.0020528126042336226, 0.002306161215528846, 0.0021315980702638626, 0.002323917346075177, 0.002990175737068057, 0.002208510646596551, 0.0022946589160710573, 0.002252590609714389, 0.0029633487574756145, 0.002340847859159112, 0.0030629110988229513, 0.003258261363953352, 0.002114378847181797, 0.003363086376339197, 0.003704913891851902, 0.00239252089522779, 0.0023953246418386698, 0.0023858528584241867, 0.0025038376916199923, 0.00347067485563457, 0.0034975032322108746, 0.002299787476658821, 0.0018222666112706065, 0.002764776349067688, 0.0024575532879680395, 0.0030361893586814404, 0.0028366968035697937, 0.002684135688468814, 0.0027327334973961115, 0.0025948300026357174, 0.0030716077890247107, 0.0030654422007501125, 0.0027327053248882294, 0.0022817100398242474, 0.0028536776080727577, 0.0027462844736874104, 0.002724277786910534, 0.002735532121732831, 0.0035156202502548695, 0.003282733028754592]\n",
      "  New scales: [0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.00390625]\n",
      "  Zero points: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[Per-channel] igates:\n",
      "  Old scales: [0.003774534910917282, 0.0023940778337419033, 0.002383454702794552, 0.002722429344430566, 0.0025774971581995487, 0.002509971847757697, 0.002434143563732505, 0.0019448098028078675, 0.003029916435480118, 0.002852785401046276, 0.0017757609020918608, 0.0019151498563587666, 0.0026379527989774942, 0.0026285056956112385, 0.002844128990545869, 0.0032900855876505375, 0.00326363043859601, 0.002135348739102483, 0.0023382671643048525, 0.0024004937149584293, 0.0025514874141663313, 0.00238351640291512, 0.002294255653396249, 0.0030489275231957436, 0.0032988376915454865, 0.0028982891235500574, 0.002887660637497902, 0.0033313073217868805, 0.002299729734659195, 0.0035172710195183754, 0.0032422151416540146, 0.003221740946173668, 0.0022547785192728043, 0.002763667143881321, 0.003448195056989789, 0.002020873362198472, 0.0036105962935835123, 0.002958093537017703, 0.003952177707105875, 0.0033217601012438536, 0.002516947453841567, 0.003081947797909379, 0.002771747065708041, 0.0036594460252672434, 0.003526166081428528, 0.0026442906819283962, 0.0025649580638855696, 0.0030064608436077833, 0.0021599186584353447, 0.002256673527881503, 0.001805945299565792, 0.0028175259940326214, 0.0019598810467869043, 0.0018946380587294698, 0.0022945941891521215, 0.002351482165977359, 0.0024410863406956196, 0.0018317075446248055, 0.0020110339391976595, 0.0028072665445506573, 0.0016798395663499832, 0.0021944616455584764, 0.001798535929992795, 0.0020701622124761343, 0.0031972466967999935, 0.002323140623047948, 0.0018376297084614635, 0.002357690827921033, 0.0014649071963503957, 0.002560941968113184, 0.002523250412195921, 0.0027052576187998056, 0.004340029321610928, 0.003500219900161028, 0.0034801962319761515, 0.003903772449120879, 0.003252949332818389, 0.0024948271457105875, 0.0034509440883994102, 0.0030005748849362135, 0.002519858069717884, 0.003074564738199115, 0.003472366137430072, 0.002495110034942627, 0.0036832948680967093, 0.0037035802379250526, 0.0036689231637865305, 0.00402648001909256, 0.003879027208313346, 0.002749479142948985, 0.0037217247299849987, 0.0027297232300043106, 0.0034615208860486746, 0.002410970162600279, 0.003639454022049904, 0.004552439786493778]\n",
      "  New scales: [0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.00390625, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.001953125, 0.00390625, 0.001953125, 0.00390625, 0.00390625]\n",
      "  Zero points: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[Per-channel] hgates:\n",
      "  Old scales: [0.003451959928497672, 0.002417167415842414, 0.001948961173184216, 0.002739359624683857, 0.002169123850762844, 0.003107663244009018, 0.0021088651847094297, 0.002705109305679798, 0.0017847326816990972, 0.0015551652759313583, 0.0024981522001326084, 0.002070418791845441, 0.002591827418655157, 0.00238125235773623, 0.0022575361654162407, 0.0020304361823946238, 0.002243315102532506, 0.0033232641872018576, 0.002073549898341298, 0.0026154243387281895, 0.00273134745657444, 0.0031875555869191885, 0.0029488210566341877, 0.0022429651580750942, 0.003756989724934101, 0.003125061048194766, 0.002307665767148137, 0.002404671162366867, 0.003031589323654771, 0.002679258119314909, 0.0031756802927702665, 0.0026348745450377464, 0.0021980791352689266, 0.002167477970942855, 0.003073749365285039, 0.0026346994563937187, 0.0022581086959689856, 0.002084307372570038, 0.0036796960048377514, 0.0034399472642689943, 0.0031572352163493633, 0.0029952707700431347, 0.002036705147475004, 0.002406517043709755, 0.002725120633840561, 0.002304739784449339, 0.0023635749239474535, 0.00207763840444386, 0.0026303441263735294, 0.0025374596007168293, 0.0022303066216409206, 0.0019206713186576962, 0.002465031808242202, 0.0022964845411479473, 0.0023118816316127777, 0.0023757731541991234, 0.002126785460859537, 0.0023383412044495344, 0.0020163378212600946, 0.001996595412492752, 0.0026727558579295874, 0.002409999957308173, 0.0024578403681516647, 0.001613677479326725, 0.003045876044780016, 0.0020769424736499786, 0.002996553434059024, 0.002374147530645132, 0.0028229537419974804, 0.0025357475969940424, 0.002057207515463233, 0.0021542354952543974, 0.0026515363715589046, 0.003133482066914439, 0.0023355965968221426, 0.0023195319809019566, 0.0035583600401878357, 0.002563048154115677, 0.002283625304698944, 0.0035641842987388372, 0.0033171053510159254, 0.003103930503129959, 0.002915899734944105, 0.0024384090211242437, 0.0038356685545295477, 0.0023143263533711433, 0.0031583751551806927, 0.0026985937729477882, 0.0037331178318709135, 0.0020465077832341194, 0.002858969150111079, 0.00334903784096241, 0.0030070804059505463, 0.0020543194841593504, 0.002963441889733076, 0.002510001650080085]\n",
      "  New scales: [0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.00390625, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.00390625, 0.001953125, 0.001953125, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.001953125, 0.00390625, 0.001953125, 0.00390625, 0.001953125, 0.00390625, 0.00390625, 0.00390625, 0.001953125, 0.00390625, 0.001953125]\n",
      "  Zero points: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
      "[Per-channel] fc:\n",
      "  Old scales: [0.005844202358275652, 0.0035232098307460546, 0.004681810736656189, 0.004921959713101387, 0.0051360223442316055, 0.0038297560531646013, 0.004974015522748232, 0.004399812780320644, 0.0060029602609574795, 0.004541982896625996]\n",
      "  New scales: [0.0078125, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.00390625, 0.0078125, 0.00390625]\n",
      "  Zero points: [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n"
     ]
    }
   ],
   "source": [
    "update_quantized_weights_and_show_scales(static_quantized_model_converted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "8b6750bf-44a9-4e6f-889b-f8cb15814dae",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate import ModelEvaluator\n",
    "print(device)\n",
    "quant_test_instance_2 = ModelEvaluator(\n",
    "    static_quantized_model_converted, \n",
    "    test_loader,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "ce129e4d-02c2-4e63-b6d8-228e603a7d18",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/ao/nn/quantizable/modules/rnn.py:543: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:167.)\n",
      "  cx_tensor = torch.stack(cx_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy on test data: 83.83%\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7174    0.9167    0.8049        72\n",
      "           1     0.8923    0.8406    0.8657        69\n",
      "           2     0.7442    0.5614    0.6400        57\n",
      "           3     0.7778    1.0000    0.8750        56\n",
      "           4     1.0000    0.9153    0.9558        59\n",
      "           5     0.8824    0.7143    0.7895        63\n",
      "           6     0.9592    0.8393    0.8952        56\n",
      "           7     0.7619    0.8727    0.8136        55\n",
      "           8     0.9615    0.8772    0.9174        57\n",
      "           9     0.7966    0.8393    0.8174        56\n",
      "\n",
      "    accuracy                         0.8383       600\n",
      "   macro avg     0.8493    0.8377    0.8374       600\n",
      "weighted avg     0.8480    0.8383    0.8371       600\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      "[[66  0  3  1  0  1  0  0  0  1]\n",
      " [ 1 58  0  0  0  1  0  9  0  0]\n",
      " [22  0 32  2  0  0  0  1  0  0]\n",
      " [ 0  0  0 56  0  0  0  0  0  0]\n",
      " [ 0  4  0  0 54  0  0  1  0  0]\n",
      " [ 1  1  2  0  0 45  0  3  0 11]\n",
      " [ 0  0  0  6  0  0 47  1  2  0]\n",
      " [ 1  2  2  1  0  1  0 48  0  0]\n",
      " [ 0  0  0  5  0  0  2  0 50  0]\n",
      " [ 1  0  4  1  0  3  0  0  0 47]]\n"
     ]
    }
   ],
   "source": [
    "quant_test_instance_2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "caeca747-03e1-4ef6-a5f2-ccf9deb44961",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_quantized_model_converted.eval()\n",
    "\n",
    "static_quant_model_save_path = os.path.join(project_root_dir, 'outputs', 'models', 'task-b-part-2_weights_jit.pth')\n",
    "\n",
    "script_model = torch.jit.script(static_quantized_model_converted)\n",
    "torch.jit.save(script_model, static_quant_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71e57e19-1515-49d5-8ad3-15d0e45c1fc7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spectgramer)",
   "language": "python",
   "name": "innatera-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
