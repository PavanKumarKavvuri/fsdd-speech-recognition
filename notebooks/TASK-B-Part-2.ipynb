{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074e24f9-a286-43cd-965b-1c41044ee99b",
   "metadata": {},
   "source": [
    "# LSTM for Spoken Digit Classification\n",
    "\n",
    "This notebook builds and trains a recurrent neural network (LSTM) to classify spoken digits (0â€“9) from audio recordings.\n",
    "\n",
    "- Dataset: [Free Spoken Digit Dataset (FSDD)](https://github.com/Jakobovski/free-spoken-digit-dataset)\n",
    "- Framework: PyTorch\n",
    "- Architecture: RNN with LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "id": "ac121864-dea8-4206-b0d1-1acbdc0002dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root (parent of current folder) to Python path\n",
    "project_root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907bf1a-f498-4c4b-941a-1ec9859a6dbc",
   "metadata": {},
   "source": [
    "## Load Model Configuration from YAML\n",
    "\n",
    "To make the training pipeline configurable and modular, we store model parameters like number of LSTM layers, hidden size, and learning rate etc in a YAML file. This structure enables quick adaptation to related tasks B, and C.\n",
    "\n",
    "This section loads the model configuration using a custom utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "id": "fc007882-2f6c-4e2e-9a19-18eb3a3dd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 288,
   "id": "f5efd2eb-0415-43c9-ba38-492a135899d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "utils.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 289,
   "id": "51de20a8-e68c-40e3-b17b-f2cc330329ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "\n",
    "model_config_path = os.path.join(project_root_dir, 'config', 'model_config.yaml')\n",
    "model_config = utils.read_yaml_file(model_config_path)\n",
    "# print(json.dumps(model_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db000b-c161-4faf-b555-80a64696bed4",
   "metadata": {},
   "source": [
    "## Load and Split Dataset for Training and Evaluation\n",
    "\n",
    "In this section, we load the recordings data from disk, generate data-label pairs, and split them into training and test sets according to the `test_size` defined in the YAML file.\n",
    "\n",
    "Using `test_size` and `seed` from the YAML config ensures that experiments are reproducible and easily tunable for other tasks by simply updating the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "fd4c30c2-fdc9-4d1f-8491-8948e8a844c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = model_config['dataset']['path']\n",
    "test_data_size = model_config['data_splitting']['test_size']\n",
    "seed = model_config['experiment']['seed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "id": "3f2e678e-b078-4b6d-8788-4807fc2912a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label_pairs, calibration_samples = utils.prepare_data_label_pairs(data_path, calibration_samples_per_class=50)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "id": "39326a52-6342-4b71-a59c-c687ac5a0005",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "500\n"
     ]
    }
   ],
   "source": [
    "print(len(calibration_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "id": "d5b6e981-6360-4c48-bf87-4d722523a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data_label_pairs, test_size=test_data_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8a06f-d118-462d-987e-c5ee7363933c",
   "metadata": {},
   "source": [
    "## Transform Raw Data into PyTorch Dataset Objects\n",
    "\n",
    "The `AudioFeaturesDataset` class converts raw data-label pairs into PyTorch-compatible datasets that provide easy access to samples and labels.\n",
    "\n",
    "AudioFeaturesDataset is a custom dataset class that:\n",
    "\n",
    "- Loads audio recordings of spoken digits along with their labels.\n",
    "- Optionally cleans the audio by filtering out noise.\n",
    "- Extracts MFCC features (a common speech feature).\n",
    "- Pads or trims these features to a fixed length so all inputs have the same shape.\n",
    "- Works with PyTorch to provide samples one-by-one when training or testing a model.\n",
    "- It helps prepare your audio data in the right format for training neural networks efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "id": "d7a181bb-fce0-4dc5-aba6-b0cc522854d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessor import AudioFeaturesDataset\n",
    "\n",
    "train_dataset = AudioFeaturesDataset(train_data)\n",
    "test_dataset = AudioFeaturesDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "4121f284-fd0e-49aa-a47b-ebfde0f1e4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2400\n",
      "Test size: 600\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682500e-8660-481b-a3a8-823d4e824486",
   "metadata": {},
   "source": [
    "## Create DataLoaders for Batch Processing\n",
    "\n",
    "Using PyTorch DataLoaders, we enable efficient loading, batching, and shuffling of data during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "a2957cdf-bee7-4877-87c8-c4eeb186b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "a30ecd48-25db-4e42-92e7-173fd8894e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = model_config['model']['input_dim']\n",
    "hidden_dim = model_config['model']['hidden_dim']\n",
    "num_layers = model_config['model']['num_layers']\n",
    "output_dim = model_config['model']['output_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "b6a3b93e-82ab-4347-99ef-5be89844a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd29cc61-758b-4310-bc8a-8d2a0513449f",
   "metadata": {},
   "source": [
    "## Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 299,
   "id": "cbcce1b9-ded8-4bcb-a57a-3fb4e7975aab",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.model as model\n",
    "\n",
    "hidden_dim = 24\n",
    "quant_model = model.LSTMClassifier(input_dim=input_dim,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       num_layers=num_layers,\n",
    "                       output_dim=output_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 300,
   "id": "9bc6c82b-ae04-4c45-9114-45b20908b919",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 300,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_constraint_model_load_path = os.path.join(project_root_dir, 'outputs', 'models', 'task-b-part-1_weights.pth')\n",
    "quant_model.load_state_dict(torch.load(memory_constraint_model_load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 301,
   "id": "c557645b-3ae1-472b-90ce-d632e4ac8476",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (lstm): LSTM(13, 24, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=24, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 301,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 302,
   "id": "74102fd8-de55-41a7-99d8-ff589f26527b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.quantize as quantize\n",
    "static_quant_model = quantize.StaticQuantizableModel(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 303,
   "id": "b0aa6bea-00c5-444c-a8f6-a832e3cb0922",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/ao/quantization/observer.py:244: UserWarning: Please use quant_min and quant_max to specify the range for observers.                     reduce_range will be deprecated in a future release of PyTorch.\n",
      "  warnings.warn(\n",
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/ao/nn/quantizable/modules/rnn.py:462: UserWarning: dropout option for quantizable LSTM is ignored. If you are training, please, use nn.LSTM version followed by `prepare` step.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "static_quant_model.qconfig = torch.quantization.get_default_qconfig('x86')  # For edge devices\n",
    "# torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "from torch.ao.quantization import MinMaxObserver, PerChannelMinMaxObserver, QConfig, MovingAverageMinMaxObserver, MovingAveragePerChannelMinMaxObserver\n",
    "qconfig = QConfig(activation=MovingAveragePerChannelMinMaxObserver.with_args(qscheme=torch.per_tensor_affine, dtype=torch.quint8),\n",
    "       weight=PerChannelMinMaxObserver.with_args(dtype=torch.qint8, qscheme=torch.per_channel_symmetric))\n",
    "\n",
    "static_quant_model_prepared = torch.quantization.prepare(static_quant_model, qconfig)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32a200f0-f392-4fad-a11a-7a2064321d6f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 304,
   "id": "285c2385-70b7-4c0d-945f-7d597de40cb2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration set size : 500\n"
     ]
    }
   ],
   "source": [
    "# calibration_indices = list(range(100))\n",
    "# calibration_dataset = torch.utils.data.Subset(test_dataset, calibration_indices)\n",
    "# calibration_dataloader = torch.utils.data.DataLoader(calibration_dataset, batch_size=32)\n",
    "\n",
    "\n",
    "calibration_sampleset = AudioFeaturesDataset(calibration_samples)\n",
    "calibration_loader = torch.utils.data.DataLoader(calibration_sampleset, batch_size=32, shuffle=True)\n",
    "print(f\"Calibration set size : {len(calibration_sampleset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 305,
   "id": "5440b3c5-07d9-4e5b-a71f-9a94ea7c21f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_quant_model_prepared_cpu = static_quant_model_prepared.to('cpu')\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "quantize.calibrate(static_quant_model_prepared, calibration_loader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "id": "702c7a4e-ca23-4526-801b-86f40fac6e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_quantized_model_converted = torch.quantization.convert(static_quant_model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "d0e93b23-1526-4397-b534-9365d9f9cd24",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate import ModelEvaluator\n",
    "print(device)\n",
    "quant_test_instance = ModelEvaluator(\n",
    "    static_quantized_model_converted, \n",
    "    test_loader,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 308,
   "id": "81f25534-4412-4767-9d1a-a5fa96700202",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/ao/nn/quantizable/modules/rnn.py:543: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:167.)\n",
      "  cx_tensor = torch.stack(cx_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy on test data: 86.33%\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.7952    0.9167    0.8516        72\n",
      "           1     0.9344    0.8261    0.8769        69\n",
      "           2     0.8478    0.6842    0.7573        57\n",
      "           3     0.7467    1.0000    0.8550        56\n",
      "           4     1.0000    0.9492    0.9739        59\n",
      "           5     0.8704    0.7460    0.8034        63\n",
      "           6     0.9800    0.8750    0.9245        56\n",
      "           7     0.7833    0.8545    0.8174        55\n",
      "           8     0.9804    0.8772    0.9259        57\n",
      "           9     0.7969    0.9107    0.8500        56\n",
      "\n",
      "    accuracy                         0.8633       600\n",
      "   macro avg     0.8735    0.8640    0.8636       600\n",
      "weighted avg     0.8736    0.8633    0.8634       600\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      "[[66  0  2  3  0  0  0  0  0  1]\n",
      " [ 1 57  2  0  0  1  0  8  0  0]\n",
      " [14  0 39  3  0  0  0  1  0  0]\n",
      " [ 0  0  0 56  0  0  0  0  0  0]\n",
      " [ 0  2  0  0 56  0  0  1  0  0]\n",
      " [ 0  1  0  0  0 47  0  3  0 12]\n",
      " [ 0  0  0  6  0  0 49  0  1  0]\n",
      " [ 2  1  1  1  0  3  0 47  0  0]\n",
      " [ 0  0  0  6  0  0  1  0 50  0]\n",
      " [ 0  0  2  0  0  3  0  0  0 51]]\n"
     ]
    }
   ],
   "source": [
    "quant_test_instance.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 309,
   "id": "a4c0e4bd-d360-41a8-ab5d-e861caed9ca0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model.lstm.layers.0.layer_fw.cell.igates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 13])\n",
      "---\n",
      "\n",
      "model.lstm.layers.0.layer_fw.cell.hgates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 24])\n",
      "---\n",
      "\n",
      "model.lstm.layers.1.layer_fw.cell.igates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 24])\n",
      "---\n",
      "\n",
      "model.lstm.layers.1.layer_fw.cell.hgates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 24])\n",
      "---\n",
      "\n",
      "model.fc is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([10, 24])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for name, module in static_quantized_model_converted.named_modules():\n",
    "    if isinstance(module, torch.nn.quantized.Linear):\n",
    "        print(f\"\\n{name} is a quantized Linear layer\")\n",
    "        weight = module.weight()\n",
    "        print(\"Weight type:\", type(weight))\n",
    "        print(\"Weight dtype:\", weight.dtype)\n",
    "        print(\"Weight shape:\", weight.shape)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 310,
   "id": "d257145a-26bd-4dba-a432-846542b292c1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated FP32 size: 32.81 KB\n",
      "Estimated INT8 size: 8.20 KB\n",
      "Compression ratio: 4.00x\n"
     ]
    }
   ],
   "source": [
    "total_fp32_bytes = 0\n",
    "total_int8_bytes = 0\n",
    "\n",
    "for name, module in static_quantized_model_converted.named_modules():\n",
    "    if isinstance(module, torch.nn.quantized.Linear):\n",
    "        weight = module.weight()\n",
    "        if weight.is_quantized:\n",
    "            num_elements = weight.numel()\n",
    "            total_fp32_bytes += num_elements * 4  # FP32\n",
    "            total_int8_bytes += num_elements * 1  # INT8\n",
    "\n",
    "print(f\"Estimated FP32 size: {total_fp32_bytes / 1024:.2f} KB\")\n",
    "print(f\"Estimated INT8 size: {total_int8_bytes / 1024:.2f} KB\")\n",
    "print(f\"Compression ratio: {total_fp32_bytes / total_int8_bytes:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 311,
   "id": "418b8710-1ae6-469e-8282-f0acbd8326d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_quant_model_save_path = os.path.join(project_root_dir, 'outputs', 'models', 'task-b-part-2_weights_no_jit.pth')\n",
    "# torch.save(static_quantized_model_converted.state_dict(), static_quant_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 312,
   "id": "d8ecd6db-6138-4599-a4f7-5301314b9d20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# static_quantized_model_converted.eval()\n",
    "\n",
    "# static_quant_model_save_path = os.path.join(project_root_dir, 'outputs', 'models', 'task-b-part-2_weights_jit.pth')\n",
    "\n",
    "# script_model = torch.jit.script(static_quantized_model_converted)\n",
    "# torch.jit.save(script_model, static_quant_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "id": "99c8ee83-1066-4fda-ba37-58fc0a163c75",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'StaticQuantizableModel' object has no attribute 'save'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[314], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mstatic_quantized_model_converted\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msave-model-test.pt\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/nn/modules/module.py:1940\u001b[0m, in \u001b[0;36mModule.__getattr__\u001b[0;34m(self, name)\u001b[0m\n\u001b[1;32m   1938\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01min\u001b[39;00m modules:\n\u001b[1;32m   1939\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m modules[name]\n\u001b[0;32m-> 1940\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\n\u001b[1;32m   1941\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mtype\u001b[39m(\u001b[38;5;28mself\u001b[39m)\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m object has no attribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m   1942\u001b[0m )\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'StaticQuantizableModel' object has no attribute 'save'"
     ]
    }
   ],
   "source": [
    "static_quantized_model_converted.save(\"save-model-test.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bb2e46b-9a22-4ea2-9fb5-4fbc4c592b73",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b84f90b-eaa3-4fff-92e2-0fe8c7f21c46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spectgramer)",
   "language": "python",
   "name": "innatera-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
