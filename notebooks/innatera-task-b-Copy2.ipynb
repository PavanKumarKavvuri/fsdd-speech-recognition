{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "074e24f9-a286-43cd-965b-1c41044ee99b",
   "metadata": {},
   "source": [
    "# LSTM for Spoken Digit Classification\n",
    "\n",
    "This notebook builds and trains a recurrent neural network (LSTM) to classify spoken digits (0–9) from audio recordings.\n",
    "\n",
    "- Dataset: [Free Spoken Digit Dataset (FSDD)](https://github.com/Jakobovski/free-spoken-digit-dataset)\n",
    "- Framework: PyTorch\n",
    "- Architecture: RNN with LSTM layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "ac121864-dea8-4206-b0d1-1acbdc0002dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "\n",
    "# Add the project root (parent of current folder) to Python path\n",
    "project_root_dir = os.path.abspath(os.path.join(os.getcwd(), '..'))\n",
    "sys.path.append(project_root_dir)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a907bf1a-f498-4c4b-941a-1ec9859a6dbc",
   "metadata": {},
   "source": [
    "## Load Model Configuration from YAML\n",
    "\n",
    "To make the training pipeline configurable and modular, we store model parameters like number of LSTM layers, hidden size, and learning rate etc in a YAML file. This structure enables quick adaptation to related tasks B, and C.\n",
    "\n",
    "This section loads the model configuration using a custom utility function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "fc007882-2f6c-4e2e-9a19-18eb3a3dd549",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.utils as utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "f5efd2eb-0415-43c9-ba38-492a135899d5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Random seed set to: 42\n"
     ]
    }
   ],
   "source": [
    "utils.set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "51de20a8-e68c-40e3-b17b-f2cc330329ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "import yaml\n",
    "import json\n",
    "\n",
    "model_config_path = os.path.join(project_root_dir, 'config', 'model_config.yaml')\n",
    "model_config = utils.read_yaml_file(model_config_path)\n",
    "# print(json.dumps(model_config, indent=2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70db000b-c161-4faf-b555-80a64696bed4",
   "metadata": {},
   "source": [
    "## Load and Split Dataset for Training and Evaluation\n",
    "\n",
    "In this section, we load the recordings data from disk, generate data-label pairs, and split them into training and test sets according to the `test_size` defined in the YAML file.\n",
    "\n",
    "Using `test_size` and `seed` from the YAML config ensures that experiments are reproducible and easily tunable for other tasks by simply updating the configuration.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "fd4c30c2-fdc9-4d1f-8491-8948e8a844c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = model_config['dataset']['path']\n",
    "test_data_size = model_config['data_splitting']['test_size']\n",
    "seed = model_config['experiment']['seed']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "3f2e678e-b078-4b6d-8788-4807fc2912a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "data_label_pairs = utils.prepare_data_label_pairs(data_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "d5b6e981-6360-4c48-bf87-4d722523a02b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "train_data, test_data = train_test_split(data_label_pairs, test_size=test_data_size, random_state=seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9e8a06f-d118-462d-987e-c5ee7363933c",
   "metadata": {},
   "source": [
    "## Transform Raw Data into PyTorch Dataset Objects\n",
    "\n",
    "The `AudioFeaturesDataset` class converts raw data-label pairs into PyTorch-compatible datasets that provide easy access to samples and labels.\n",
    "\n",
    "AudioFeaturesDataset is a custom dataset class that:\n",
    "\n",
    "- Loads audio recordings of spoken digits along with their labels.\n",
    "- Optionally cleans the audio by filtering out noise.\n",
    "- Extracts MFCC features (a common speech feature).\n",
    "- Pads or trims these features to a fixed length so all inputs have the same shape.\n",
    "- Works with PyTorch to provide samples one-by-one when training or testing a model.\n",
    "- It helps prepare your audio data in the right format for training neural networks efficiently.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "d7a181bb-fce0-4dc5-aba6-b0cc522854d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data_preprocessor import AudioFeaturesDataset\n",
    "\n",
    "train_dataset = AudioFeaturesDataset(train_data)\n",
    "test_dataset = AudioFeaturesDataset(test_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "4121f284-fd0e-49aa-a47b-ebfde0f1e4a6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train size: 2400\n",
      "Test size: 600\n"
     ]
    }
   ],
   "source": [
    "print(f\"Train size: {len(train_dataset)}\")\n",
    "print(f\"Test size: {len(test_dataset)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0682500e-8660-481b-a3a8-823d4e824486",
   "metadata": {},
   "source": [
    "## Create DataLoaders for Batch Processing\n",
    "\n",
    "Using PyTorch DataLoaders, we enable efficient loading, batching, and shuffling of data during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "id": "a2957cdf-bee7-4877-87c8-c4eeb186b190",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_dataset, batch_size=32, shuffle=True)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "533f7c9b-69cc-4239-837f-c1c82e9f3695",
   "metadata": {},
   "source": [
    "## LSTM Model Definition\n",
    "\n",
    "A simple `n`-layer LSTM followed by a fully connected output layer. Variable `n` is defined in the configuration YAML file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "a30ecd48-25db-4e42-92e7-173fd8894e28",
   "metadata": {},
   "outputs": [],
   "source": [
    "input_dim = model_config['model']['input_dim']\n",
    "hidden_dim = model_config['model']['hidden_dim']\n",
    "num_layers = model_config['model']['num_layers']\n",
    "output_dim = model_config['model']['output_dim']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "b6a3b93e-82ab-4347-99ef-5be89844a4c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "acc6726e-a78c-4846-900e-d1f38377c3a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.model as model\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "\n",
    "float_model = model.LSTMClassifier(input_dim=input_dim,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       num_layers=num_layers,\n",
    "                       output_dim=output_dim).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8b334d1-1991-454e-9b55-4621c7c63362",
   "metadata": {},
   "source": [
    "## Training Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "3b4bcea8-c7ff-46d4-b391-78c08e05adaa",
   "metadata": {},
   "outputs": [],
   "source": [
    "learning_rate = model_config['training']['learning_rate']\n",
    "epochs = model_config['training']['epochs']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "59d97af0-0fa0-44c6-a66e-68805db8fc70",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import ModelTrainer\n",
    "trainer_instance = ModelTrainer(\n",
    "    float_model, \n",
    "    epochs,\n",
    "    train_loader,\n",
    "    device,\n",
    "    learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "b93e3b2c-3a36-4320-99a8-4ec297bd853a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 134.0127, Accuracy: 57.67%\n",
      "Epoch [2/20], Loss: 49.1018, Accuracy: 86.17%\n",
      "Epoch [3/20], Loss: 21.3893, Accuracy: 96.75%\n",
      "Epoch [4/20], Loss: 12.2558, Accuracy: 96.42%\n",
      "Epoch [5/20], Loss: 9.3282, Accuracy: 98.12%\n",
      "Epoch [6/20], Loss: 6.7593, Accuracy: 96.42%\n",
      "Epoch [7/20], Loss: 5.2892, Accuracy: 98.58%\n",
      "Epoch [8/20], Loss: 4.0651, Accuracy: 99.17%\n",
      "Epoch [9/20], Loss: 1.7189, Accuracy: 99.33%\n",
      "Epoch [10/20], Loss: 1.9559, Accuracy: 99.79%\n",
      "Epoch [11/20], Loss: 3.8065, Accuracy: 95.79%\n",
      "Epoch [12/20], Loss: 3.7462, Accuracy: 99.79%\n",
      "Epoch [13/20], Loss: 1.1268, Accuracy: 99.79%\n",
      "Epoch [14/20], Loss: 1.7250, Accuracy: 99.08%\n",
      "Epoch [15/20], Loss: 2.5933, Accuracy: 99.62%\n",
      "Epoch [16/20], Loss: 2.1036, Accuracy: 99.25%\n",
      "Epoch [17/20], Loss: 1.4843, Accuracy: 99.67%\n",
      "Epoch [18/20], Loss: 0.4845, Accuracy: 99.92%\n",
      "Epoch [19/20], Loss: 0.8350, Accuracy: 100.00%\n",
      "Epoch [20/20], Loss: 0.1467, Accuracy: 100.00%\n"
     ]
    }
   ],
   "source": [
    "trainer_instance.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "id": "dbd17cd3-e337-43b2-b8fd-08297ef05941",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer-wise parameter counts:\n",
      "lstm.weight_ih_l0              -> 6,656 params\n",
      "lstm.weight_hh_l0              -> 65,536 params\n",
      "lstm.bias_ih_l0                -> 512 params\n",
      "lstm.bias_hh_l0                -> 512 params\n",
      "lstm.weight_ih_l1              -> 65,536 params\n",
      "lstm.weight_hh_l1              -> 65,536 params\n",
      "lstm.bias_ih_l1                -> 512 params\n",
      "lstm.bias_hh_l1                -> 512 params\n",
      "fc.weight                      -> 1,280 params\n",
      "fc.bias                        -> 10 params\n",
      "\n",
      "\n",
      " Total Parameters: 206,602\n",
      "Estimated Memory: 807.04 KB (0.79 MB)\n"
     ]
    }
   ],
   "source": [
    "_, _ = utils.get_model_params_size(float_model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e192852-2d93-484f-b4f1-69f4689d730f",
   "metadata": {},
   "source": [
    "## Evaluation & Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "id": "85f947da-ba60-44fc-9d7d-1554804a6f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import ModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "id": "2ef20b9c-08b9-42c9-ae8b-93b7e7c4f295",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance = ModelEvaluator(\n",
    "    float_model, \n",
    "    test_loader,\n",
    "    device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "id": "52e00d21-ba5c-4cdb-986a-978ccc3d0d93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy on test data: 99.17%\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     1.0000    1.0000    1.0000        52\n",
      "           1     1.0000    1.0000    1.0000        65\n",
      "           2     0.9692    1.0000    0.9844        63\n",
      "           3     1.0000    0.9667    0.9831        60\n",
      "           4     1.0000    1.0000    1.0000        55\n",
      "           5     1.0000    0.9811    0.9905        53\n",
      "           6     1.0000    0.9846    0.9922        65\n",
      "           7     1.0000    0.9831    0.9915        59\n",
      "           8     0.9683    1.0000    0.9839        61\n",
      "           9     0.9853    1.0000    0.9926        67\n",
      "\n",
      "    accuracy                         0.9917       600\n",
      "   macro avg     0.9923    0.9915    0.9918       600\n",
      "weighted avg     0.9919    0.9917    0.9917       600\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      "[[52  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 65  0  0  0  0  0  0  0  0]\n",
      " [ 0  0 63  0  0  0  0  0  0  0]\n",
      " [ 0  0  1 58  0  0  0  0  1  0]\n",
      " [ 0  0  0  0 55  0  0  0  0  0]\n",
      " [ 0  0  0  0  0 52  0  0  0  1]\n",
      " [ 0  0  0  0  0  0 64  0  1  0]\n",
      " [ 0  0  1  0  0  0  0 58  0  0]\n",
      " [ 0  0  0  0  0  0  0  0 61  0]\n",
      " [ 0  0  0  0  0  0  0  0  0 67]]\n"
     ]
    }
   ],
   "source": [
    "test_instance.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "id": "7233c710-d50f-481e-a346-72a8eb705976",
   "metadata": {},
   "outputs": [],
   "source": [
    "float_model_save_path = os.path.join(project_root_dir, 'outputs', 'models', 'float_model_weights.pth')\n",
    "torch.save(float_model.state_dict(), float_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "477f5d7b-7c28-434e-a707-14d1da84701e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lstm.weight_ih_l0: dtype=torch.float32\n",
      "lstm.weight_hh_l0: dtype=torch.float32\n",
      "lstm.bias_ih_l0: dtype=torch.float32\n",
      "lstm.bias_hh_l0: dtype=torch.float32\n",
      "lstm.weight_ih_l1: dtype=torch.float32\n",
      "lstm.weight_hh_l1: dtype=torch.float32\n",
      "lstm.bias_ih_l1: dtype=torch.float32\n",
      "lstm.bias_hh_l1: dtype=torch.float32\n",
      "fc.weight: dtype=torch.float32\n",
      "fc.bias: dtype=torch.float32\n"
     ]
    }
   ],
   "source": [
    "for name, param in float_model.named_parameters():\n",
    "    print(f\"{name}: dtype={param.dtype}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f863b267-4316-4fe2-8a49-72d03ea4fbd3",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Task B — Retrain Under Memory Constraints\n",
    "\n",
    "- All parameters of any one layer must fit into memory simultaneously\n",
    "- Maximum memory available for layer parameters is 36 KB\n",
    "\n",
    "Since Pytorch stores all the layer parameters as floating point values, as 32-bit floats(4 bytes per parameter), this implies that the \n",
    "maximum number of parameters should not exceed\n",
    "\n",
    "$$\n",
    "\\text{Max total number of parameters} = \\frac{36\\,\\text{KB}}{4\\,\\text{bytes}} = \\frac{36 \\times 1024}{4} = 9,216 \\text{ parameters}\n",
    "$$\n",
    "\n",
    "## Model Parameter Breakdown for 2 Layers LSTM\n",
    "The following calculations are based on the parameter definitions from PyTorch's LSTM implementation, as described in the \\href{https://docs.pytorch.org/docs/stable/generated/torch.nn.LSTM.html\\#torch.nn.LSTM}{official documentation}. \n",
    "\n",
    "Let’s define the following variables:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "\\text{Input dimension} &= i \\\\\n",
    "\\text{Hidden dimension} &= h \\\\\n",
    "\\text{Output dimension} &= o \\\\\n",
    "\\text{Number of LSTM layers} &= 2 \\\\\n",
    "\\text{Fully Connected (Linear) layer count} &= 1 \\\\\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "These will be used to calculate the total number of parameters in the model.\n",
    "\n",
    "---\n",
    "\n",
    "## LSTM Layer Parameters\n",
    "\n",
    "Each LSTM layer has 4 internal gates (input, forget, cell, output).  \n",
    "So each layer has:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "W\\_{ih} &: \\text{Weights from input to hidden} \\rightarrow \\text{shape: } (4 \\times h, i) \\\\\n",
    "W\\_{hh} &: \\text{Weights from hidden to hidden} \\rightarrow \\text{shape: } (4 \\times h, h) \\\\\n",
    "b\\_{ih}, b\\_{hh} &: \\text{Biases for each gate} \\rightarrow \\text{shape: } (4 \\times h)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "### Calculations:\n",
    "\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\textbf{Layer 1 Parameters} \\\\\n",
    "&W\\_ih: 4 \\times h \\times i \\\\\n",
    "&W\\_hh: 4 \\times h \\times h \\\\\n",
    "&b\\_ih: 4 \\times h \\\\\n",
    "&b\\_hh: 4 \\times h \\\\\n",
    "&\\\\\n",
    "&\\textbf{Layer 2 Parameters} \\\\\n",
    "&W\\_ih: 4 \\times h \\times h \\\\\n",
    "&W\\_hh: 4 \\times h \\times h \\\\\n",
    "&b\\_ih: 4 \\times h \\\\\n",
    "&b\\_hh: 4 \\times h \\\\\n",
    "&\\\\\n",
    "&\\text{Total LSTM parameters} = (4 \\times h \\times i) + (4 \\times h \\times h) + (8 \\times h) = 4hi + 4h^2 + 8h\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Fully Connected (Linear) Layer\n",
    "\n",
    "- Input features = $$\\text{hidden\\_dim} = h$$  \n",
    "- Output features = $$\\text{output\\_dim} = o$$\n",
    "\n",
    "### Calculations:\n",
    "$$\n",
    "\\begin{aligned}\n",
    "&\\text{Weights}: \\quad h \\times o \\\\\n",
    "&\\text{Bias}: \\quad o \\\\\n",
    "&\\textbf{Total Linear parameters} = h \\cdot o + o = o(h + 1)\n",
    "\\end{aligned}\n",
    "$$\n",
    "\n",
    "---\n",
    "\n",
    "## Total Parameters\n",
    "\n",
    "The total number of parameters in the model is the sum of the LSTM and Linear layer parameters:\n",
    "\n",
    "$$\n",
    "\\text{Total Parameters} = \\text{Total LSTM Parameters} + \\text{Total Linear Parameters}\n",
    "$$\n",
    "\n",
    "From earlier calculations:\n",
    "\n",
    "- $\\text{Total LSTM Parameters} = 4hi + 4h^2 + 8h$\n",
    "- $\\text{Total Linear Parameters} = h \\cdot o + o = o(h + 1)$\n",
    "\n",
    "Therefore:\n",
    "\n",
    "$$\n",
    "\\text{Total Parameters} = (4hi + 4h^2 + 8h) + o(h + 1)\n",
    "$$\n",
    "\n",
    "Or more compactly:\n",
    "\n",
    "$$\n",
    "\\boxed{\\text{Total Parameters} = 4hi + 4h^2 + 8h + o(h + 1)}\n",
    "$$\n",
    "\n",
    "---\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e958d7b7-41d4-4753-99c0-42ae9303be61",
   "metadata": {},
   "source": [
    "## Designing a 2-Layer LSTM Under a 36 KB Memory Constraint\n",
    "\n",
    "Given an output dimension of $10$ (representing 10 classes or digits) and an input dimension of $13$ (corresponding to 13 MFCC coefficients per time step), the total number of parameters in the model reduces to the following quadratic expression:\n",
    "\n",
    "$$\n",
    "\\text{Total Parameters} = 12h^2 + 78h + 10\n",
    "$$\n",
    "\n",
    "Here, $ h $ (the hidden dimension) remains the only variable we need to solve for.\n",
    "\n",
    "Since the memory constraint allows for a maximum of \\textbf{9,216 parameters}, the hidden dimension must satisfy:\n",
    "\n",
    "$$\n",
    "12h^2 + 78h + 10 \\leq 9216\n",
    "$$\n",
    "\n",
    "The maximum valid integer value of $ h $ that satisfies the inequality is:\n",
    "$$\n",
    "h = 24\n",
    "$$\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "id": "92bbbd4b-4632-472a-885e-2e07f1894c7d",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.model as model_with_constraints\n",
    "\n",
    "hidden_dim = 24\n",
    "memory_constraint_model = model_with_constraints.LSTMClassifier(input_dim=input_dim,\n",
    "                                           hidden_dim=hidden_dim,\n",
    "                                           num_layers=num_layers,\n",
    "                                           output_dim=output_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "bf734e3a-c8ec-4df5-98e8-22a38e69674e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.train import ModelTrainer\n",
    "trainer_instance_2 = ModelTrainer(\n",
    "    memory_constraint_model, \n",
    "    epochs,\n",
    "    train_loader,\n",
    "    device,\n",
    "    learning_rate\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "id": "16108f19-ba8a-42ee-85fd-d8b9dc257643",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/20], Loss: 172.7911, Accuracy: 14.88%\n",
      "Epoch [2/20], Loss: 161.7900, Accuracy: 32.71%\n",
      "Epoch [3/20], Loss: 122.7087, Accuracy: 58.21%\n",
      "Epoch [4/20], Loss: 93.8751, Accuracy: 70.17%\n",
      "Epoch [5/20], Loss: 71.3963, Accuracy: 77.58%\n",
      "Epoch [6/20], Loss: 58.7306, Accuracy: 84.46%\n",
      "Epoch [7/20], Loss: 54.2336, Accuracy: 83.71%\n",
      "Epoch [8/20], Loss: 40.8367, Accuracy: 89.00%\n",
      "Epoch [9/20], Loss: 34.6832, Accuracy: 91.25%\n",
      "Epoch [10/20], Loss: 30.1345, Accuracy: 85.88%\n",
      "Epoch [11/20], Loss: 31.7366, Accuracy: 92.08%\n",
      "Epoch [12/20], Loss: 23.9289, Accuracy: 94.58%\n",
      "Epoch [13/20], Loss: 20.8943, Accuracy: 94.79%\n",
      "Epoch [14/20], Loss: 18.5835, Accuracy: 96.04%\n",
      "Epoch [15/20], Loss: 16.9845, Accuracy: 95.83%\n",
      "Epoch [16/20], Loss: 14.5756, Accuracy: 96.62%\n",
      "Epoch [17/20], Loss: 13.4785, Accuracy: 96.67%\n",
      "Epoch [18/20], Loss: 15.8476, Accuracy: 97.04%\n",
      "Epoch [19/20], Loss: 11.0573, Accuracy: 96.92%\n",
      "Epoch [20/20], Loss: 10.5518, Accuracy: 97.96%\n"
     ]
    }
   ],
   "source": [
    "trainer_instance_2.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "id": "6ad2f6cf-3eb9-48c2-9152-7843f556a770",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer-wise parameter counts:\n",
      "lstm.weight_ih_l0              -> 1,248 params\n",
      "lstm.weight_hh_l0              -> 2,304 params\n",
      "lstm.bias_ih_l0                -> 96 params\n",
      "lstm.bias_hh_l0                -> 96 params\n",
      "lstm.weight_ih_l1              -> 2,304 params\n",
      "lstm.weight_hh_l1              -> 2,304 params\n",
      "lstm.bias_ih_l1                -> 96 params\n",
      "lstm.bias_hh_l1                -> 96 params\n",
      "fc.weight                      -> 240 params\n",
      "fc.bias                        -> 10 params\n",
      "\n",
      "\n",
      " Total Parameters: 8,794\n",
      "Estimated Memory: 34.35 KB (0.03 MB)\n"
     ]
    }
   ],
   "source": [
    "_, _ = utils.get_model_params_size(memory_constraint_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "id": "7bace95f-b096-4648-baf2-2bdc0375f9bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import ModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "91968fee-a54e-40ba-a2ed-1ba2d1039615",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_instance_2 = ModelEvaluator(\n",
    "    memory_constraint_model, \n",
    "    test_loader,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "id": "4352fe35-66bc-4cf1-8bd3-d129e20617e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy on test data: 96.50%\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.9811    1.0000    0.9905        52\n",
      "           1     1.0000    1.0000    1.0000        65\n",
      "           2     0.9839    0.9683    0.9760        63\n",
      "           3     0.9333    0.9333    0.9333        60\n",
      "           4     0.9818    0.9818    0.9818        55\n",
      "           5     0.9636    1.0000    0.9815        53\n",
      "           6     0.8939    0.9077    0.9008        65\n",
      "           7     0.9831    0.9831    0.9831        59\n",
      "           8     0.9333    0.9180    0.9256        61\n",
      "           9     1.0000    0.9701    0.9848        67\n",
      "\n",
      "    accuracy                         0.9650       600\n",
      "   macro avg     0.9654    0.9662    0.9657       600\n",
      "weighted avg     0.9652    0.9650    0.9650       600\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      "[[52  0  0  0  0  0  0  0  0  0]\n",
      " [ 0 65  0  0  0  0  0  0  0  0]\n",
      " [ 1  0 61  0  1  0  0  0  0  0]\n",
      " [ 0  0  1 56  0  0  2  1  0  0]\n",
      " [ 0  0  0  0 54  0  1  0  0  0]\n",
      " [ 0  0  0  0  0 53  0  0  0  0]\n",
      " [ 0  0  0  2  0  0 59  0  4  0]\n",
      " [ 0  0  0  0  0  0  1 58  0  0]\n",
      " [ 0  0  0  2  0  0  3  0 56  0]\n",
      " [ 0  0  0  0  0  2  0  0  0 65]]\n"
     ]
    }
   ],
   "source": [
    "test_instance_2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "418b8710-1ae6-469e-8282-f0acbd8326d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_constraint_model_save_path = os.path.join(project_root_dir, 'outputs', 'models', 'memory_constraint_model_weights.pth')\n",
    "torch.save(memory_constraint_model.state_dict(), memory_constraint_model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "0fba6210-ab20-4ca3-82d0-f084dcbfe595",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Memory Constraint model size: 37.13 KB\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=True) as tmp:\n",
    "    torch.save(memory_constraint_model.state_dict(), tmp.name)\n",
    "    memory_model_size_kb = os.path.getsize(tmp.name) / 1024\n",
    "    print(f\"Memory Constraint model size: {memory_model_size_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb4fd5d7-b633-4659-8782-d071a75553af",
   "metadata": {},
   "source": [
    "## Floating Point Restriction (Task B Continued)\n",
    "\n",
    "In addition to the 36 KB memory constraint, the hardware for Task B is limited in computational capability and **does not support floating point operations**. It means the network must operate using **integer or fixed-point arithmetic** only. **Weights, activations, and computations** should be **quantized** to lower-precision formats such as 8-bit integers (INT8)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1bde340-b0ed-4641-9fd6-c383c328d121",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Dynamic Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "65bbcbb6-07e7-432e-bf04-d56cf0d20471",
   "metadata": {},
   "outputs": [],
   "source": [
    "memory_constraint_model_cpu = memory_constraint_model.to('cpu') \n",
    "# Apply dynamic quantization to the entire model or just LSTM/Linear layers\n",
    "dynamic_quantized_model = torch.quantization.quantize_dynamic(\n",
    "    memory_constraint_model_cpu,  # the model instance\n",
    "    {nn.LSTM, nn.Linear},  # layers to quantize\n",
    "    dtype=torch.qint8  # quantize to 8-bit integers\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "40d46328-0968-47c0-a43e-fa9c9cd95d45",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import ModelEvaluator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "4ed358da-c959-47ba-b90e-1dac930cf08a",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device(\"cpu\")\n",
    "test_instance = ModelEvaluator(\n",
    "    memory_constraint_model_cpu, \n",
    "    test_loader,\n",
    "    device\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "cde78786-f9d9-4f73-a478-77e46158c359",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy on test data: 58.50%\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.5882    0.7692    0.6667        52\n",
      "           1     0.0000    0.0000    0.0000        65\n",
      "           2     0.5625    0.4286    0.4865        63\n",
      "           3     0.7931    0.7667    0.7797        60\n",
      "           4     0.6250    0.4545    0.5263        55\n",
      "           5     0.7193    0.7736    0.7455        53\n",
      "           6     0.7424    0.7538    0.7481        65\n",
      "           7     0.3617    0.8644    0.5100        59\n",
      "           8     0.6716    0.7377    0.7031        61\n",
      "           9     0.4909    0.4030    0.4426        67\n",
      "\n",
      "    accuracy                         0.5850       600\n",
      "   macro avg     0.5555    0.5952    0.5608       600\n",
      "weighted avg     0.5493    0.5850    0.5530       600\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      "[[40  0  4  1  2  0  0  1  0  4]\n",
      " [ 0  0  0  0  0  9  0 46  0 10]\n",
      " [20  0 27  1  7  1  1  0  1  5]\n",
      " [ 0  0  2 46  4  0  1  0  7  0]\n",
      " [ 8  0 14  4 25  0  0  0  0  4]\n",
      " [ 0  0  0  0  0 41  2  8  0  2]\n",
      " [ 0  0  0  2  0  0 49  0 14  0]\n",
      " [ 0  0  0  0  0  4  1 51  0  3]\n",
      " [ 0  0  0  4  0  0 12  0 45  0]\n",
      " [ 0  0  1  0  2  2  0 35  0 27]]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
     ]
    }
   ],
   "source": [
    "test_instance.evaluate()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "458c9697-dd05-4536-ab79-44a9013ad2af",
   "metadata": {},
   "source": [
    "#### Compare the Inference times for Floating point and Quantised model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a490e43a-6b02-40e8-8195-9ac033d0e327",
   "metadata": {},
   "source": [
    "#### Compare the model sizes of Floating point and Quantised model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c4f7307-719c-42f2-94b8-ed65a510a0f2",
   "metadata": {},
   "source": [
    "### Static Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "e4cfe0dd-e157-48aa-b670-6864307591e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.model as model\n",
    "\n",
    "hidden_dim = 24\n",
    "quant_model = model.LSTMClassifier(input_dim=input_dim,\n",
    "                       hidden_dim=hidden_dim,\n",
    "                       num_layers=num_layers,\n",
    "                       output_dim=output_dim).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9df73ae4-aa64-4a8d-9abe-bf1b8146d8d3",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "memory_constraint_model_load_path = os.path.join(project_root_dir, 'outputs', 'models', 'memory_constraint_model_weights.pth')\n",
    "quant_model.load_state_dict(torch.load(memory_constraint_model_load_path))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "c2dbbef6-8eac-4ae4-9c47-42c5088e6946",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LSTMClassifier(\n",
       "  (lstm): LSTM(13, 24, num_layers=2, batch_first=True, dropout=0.3)\n",
       "  (dropout): Dropout(p=0.3, inplace=False)\n",
       "  (fc): Linear(in_features=24, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "quant_model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "468d1b42-cdcf-43db-b4e3-8e2c3dbc3120",
   "metadata": {},
   "outputs": [],
   "source": [
    "import src.quantize as quantize\n",
    "static_quant_model = quantize.StaticQuantizableModel(quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "8cd8b6e6-4df2-45b3-b4ee-f9e77afe362c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StaticQuantizableModel(\n",
      "  (quant): QuantStub()\n",
      "  (dequant): DeQuantStub()\n",
      "  (model): LSTMClassifier(\n",
      "    (lstm): LSTM(13, 24, num_layers=2, batch_first=True, dropout=0.3)\n",
      "    (dropout): Dropout(p=0.3, inplace=False)\n",
      "    (fc): Linear(in_features=24, out_features=10, bias=True)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(static_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "47bddac4-37d9-4954-bd97-673550566bf7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'torch' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m static_quant_model\u001b[38;5;241m.\u001b[39mqconfig \u001b[38;5;241m=\u001b[39m \u001b[43mtorch\u001b[49m\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mget_default_qconfig(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfbgemm\u001b[39m\u001b[38;5;124m'\u001b[39m)  \u001b[38;5;66;03m# For edge devices\u001b[39;00m\n\u001b[1;32m      2\u001b[0m torch\u001b[38;5;241m.\u001b[39mbackends\u001b[38;5;241m.\u001b[39mquantized\u001b[38;5;241m.\u001b[39mengine \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfbgemm\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m      4\u001b[0m static_quant_model_prepared \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mquantization\u001b[38;5;241m.\u001b[39mprepare(static_quant_model)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
     ]
    }
   ],
   "source": [
    "static_quant_model.qconfig = torch.quantization.get_default_qconfig('fbgemm')  # For edge devices\n",
    "torch.backends.quantized.engine = 'fbgemm'\n",
    "\n",
    "static_quant_model_prepared = torch.quantization.prepare(static_quant_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "5e533a1d-2ade-4e1a-9c4a-8549532aad15",
   "metadata": {},
   "outputs": [],
   "source": [
    "calibration_indices = list(range(100))\n",
    "calibration_dataset = torch.utils.data.Subset(test_dataset, calibration_indices)\n",
    "\n",
    "calibration_dataloader = torch.utils.data.DataLoader(calibration_dataset, batch_size=32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "7074be83-59fd-4dcb-8d1e-6d0b928d934f",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_quant_model_prepared = static_quant_model_prepared.to('cpu')\n",
    "device = torch.device(\"cpu\")\n",
    "\n",
    "quantize.calibrate(static_quant_model_prepared, calibration_dataloader, device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "344dadd0-283b-4cbc-b6d5-9795a7e11a7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_quantized_model = torch.quantization.convert(static_quant_model_prepared)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "ee63cb0d-921f-4d81-abc5-93730e69dc76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9e360937-58a0-4dd4-84d4-0f2a9cc960f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "from src.evaluate import ModelEvaluator\n",
    "print(device)\n",
    "quant_test_instance = ModelEvaluator(\n",
    "    static_quantized_model, \n",
    "    test_loader,\n",
    "    device\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "116e9487-62c1-429b-9716-e3a5def9f8fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/ao/nn/quantizable/modules/rnn.py:542: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:167.)\n",
      "  hx_tensor = torch.stack(hx_list)\n",
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/ao/nn/quantizable/modules/rnn.py:543: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:167.)\n",
      "  cx_tensor = torch.stack(cx_list)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " Accuracy on test data: 51.83%\n",
      "\n",
      " Classification Report:\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0     0.4271    0.7885    0.5541        52\n",
      "           1     0.3077    0.0615    0.1026        65\n",
      "           2     0.1000    0.0159    0.0274        63\n",
      "           3     0.7121    0.7833    0.7460        60\n",
      "           4     0.7500    0.3818    0.5060        55\n",
      "           5     0.7674    0.6226    0.6875        53\n",
      "           6     0.8000    0.4923    0.6095        65\n",
      "           7     0.3535    0.5932    0.4430        59\n",
      "           8     0.5600    0.9180    0.6957        61\n",
      "           9     0.3905    0.6119    0.4767        67\n",
      "\n",
      "    accuracy                         0.5183       600\n",
      "   macro avg     0.5168    0.5269    0.4849       600\n",
      "weighted avg     0.5106    0.5183    0.4773       600\n",
      "\n",
      "\n",
      " Confusion Matrix:\n",
      "[[41  0  1  1  3  0  0  3  1  2]\n",
      " [ 0  4  0  1  0  6  0 25  0 29]\n",
      " [36  0  1  6  4  2  0  2  1 11]\n",
      " [ 1  0  0 47  0  0  0  0 11  1]\n",
      " [18  0  8  4 21  0  0  1  0  3]\n",
      " [ 0  0  0  0  0 33  5 11  0  4]\n",
      " [ 0  0  0  2  0  0 32  0 31  0]\n",
      " [ 0  7  0  1  0  1  1 35  0 14]\n",
      " [ 0  0  0  3  0  0  2  0 56  0]\n",
      " [ 0  2  0  1  0  1  0 22  0 41]]\n"
     ]
    }
   ],
   "source": [
    "quant_test_instance.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "3429977c-2248-4321-879b-4912694ef906",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/ao/nn/quantizable/modules/rnn.py:542: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:167.)\n",
      "  hx_tensor = torch.stack(hx_list)\n",
      "/home/pavan/Music/spectrum/spectgramer/lib/python3.10/site-packages/torch/ao/nn/quantizable/modules/rnn.py:543: UserWarning: All inputs of this cat operator must share the same quantization parameters. Otherwise large numerical inaccuracies may occur. (Triggered internally at /pytorch/aten/src/ATen/native/quantized/cpu/TensorShape.cpp:167.)\n",
      "  cx_tensor = torch.stack(cx_list)\n"
     ]
    }
   ],
   "source": [
    "static_quantized_model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "\n",
    "    for inputs, labels in test_loader:\n",
    "        # inputs = inputs.to(device)\n",
    "        labels = labels.to(device)\n",
    "\n",
    "        outputs = static_quantized_model(inputs)\n",
    "        _, predicted = torch.max(outputs, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "c5575187-1d97-4867-8cad-06e53ece60aa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Layer-wise parameter counts:\n",
      "\n",
      "\n",
      " Total Parameters: 0\n",
      "Estimated Memory: 0.00 KB (0.00 MB)\n"
     ]
    }
   ],
   "source": [
    "_, _ = utils.get_model_params_size(static_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "3fbc02fb-3738-40ee-a933-1baffb33aa6d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Quantized model size: 35.48 KB\n"
     ]
    }
   ],
   "source": [
    "import tempfile\n",
    "\n",
    "with tempfile.NamedTemporaryFile(delete=True) as tmp:\n",
    "    torch.save(static_quantized_model.state_dict(), tmp.name)\n",
    "    quantized_model_size_kb = os.path.getsize(tmp.name) / 1024\n",
    "    print(f\"Quantized model size: {quantized_model_size_kb:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "da5f7ca8-6065-45d2-94fb-15b428c21340",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer:  | Type: <class 'src.quantize.StaticQuantizableModel'>\n",
      "Layer: quant | Type: <class 'torch.ao.nn.quantized.modules.Quantize'>\n",
      "Layer: dequant | Type: <class 'torch.ao.nn.quantized.modules.DeQuantize'>\n",
      "Layer: model | Type: <class 'src.model.LSTMClassifier'>\n",
      "Layer: model.lstm | Type: <class 'torch.ao.nn.quantized.modules.rnn.LSTM'>\n",
      "Layer: model.lstm.layers | Type: <class 'torch.nn.modules.container.ModuleList'>\n",
      "Layer: model.lstm.layers.0 | Type: <class 'torch.ao.nn.quantizable.modules.rnn._LSTMLayer'>\n",
      "Layer: model.lstm.layers.0.layer_fw | Type: <class 'torch.ao.nn.quantizable.modules.rnn._LSTMSingleLayer'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell | Type: <class 'torch.ao.nn.quantizable.modules.rnn.LSTMCell'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.igates | Type: <class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.igates._packed_params | Type: <class 'torch.ao.nn.quantized.modules.linear.LinearPackedParams'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.hgates | Type: <class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.hgates._packed_params | Type: <class 'torch.ao.nn.quantized.modules.linear.LinearPackedParams'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.gates | Type: <class 'torch.ao.nn.quantized.modules.functional_modules.QFunctional'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.gates.activation_post_process | Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.input_gate | Type: <class 'torch.nn.modules.activation.Sigmoid'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.forget_gate | Type: <class 'torch.nn.modules.activation.Sigmoid'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.cell_gate | Type: <class 'torch.nn.modules.activation.Tanh'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.output_gate | Type: <class 'torch.nn.modules.activation.Sigmoid'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.fgate_cx | Type: <class 'torch.ao.nn.quantized.modules.functional_modules.QFunctional'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.fgate_cx.activation_post_process | Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.igate_cgate | Type: <class 'torch.ao.nn.quantized.modules.functional_modules.QFunctional'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.igate_cgate.activation_post_process | Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.fgate_cx_igate_cgate | Type: <class 'torch.ao.nn.quantized.modules.functional_modules.QFunctional'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.fgate_cx_igate_cgate.activation_post_process | Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.ogate_cy | Type: <class 'torch.ao.nn.quantized.modules.functional_modules.QFunctional'>\n",
      "Layer: model.lstm.layers.0.layer_fw.cell.ogate_cy.activation_post_process | Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer: model.lstm.layers.1 | Type: <class 'torch.ao.nn.quantizable.modules.rnn._LSTMLayer'>\n",
      "Layer: model.lstm.layers.1.layer_fw | Type: <class 'torch.ao.nn.quantizable.modules.rnn._LSTMSingleLayer'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell | Type: <class 'torch.ao.nn.quantizable.modules.rnn.LSTMCell'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.igates | Type: <class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.igates._packed_params | Type: <class 'torch.ao.nn.quantized.modules.linear.LinearPackedParams'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.hgates | Type: <class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.hgates._packed_params | Type: <class 'torch.ao.nn.quantized.modules.linear.LinearPackedParams'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.gates | Type: <class 'torch.ao.nn.quantized.modules.functional_modules.QFunctional'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.gates.activation_post_process | Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.input_gate | Type: <class 'torch.nn.modules.activation.Sigmoid'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.forget_gate | Type: <class 'torch.nn.modules.activation.Sigmoid'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.cell_gate | Type: <class 'torch.nn.modules.activation.Tanh'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.output_gate | Type: <class 'torch.nn.modules.activation.Sigmoid'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.fgate_cx | Type: <class 'torch.ao.nn.quantized.modules.functional_modules.QFunctional'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.fgate_cx.activation_post_process | Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.igate_cgate | Type: <class 'torch.ao.nn.quantized.modules.functional_modules.QFunctional'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.igate_cgate.activation_post_process | Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.fgate_cx_igate_cgate | Type: <class 'torch.ao.nn.quantized.modules.functional_modules.QFunctional'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.fgate_cx_igate_cgate.activation_post_process | Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.ogate_cy | Type: <class 'torch.ao.nn.quantized.modules.functional_modules.QFunctional'>\n",
      "Layer: model.lstm.layers.1.layer_fw.cell.ogate_cy.activation_post_process | Type: <class 'torch.nn.modules.linear.Identity'>\n",
      "Layer: model.dropout | Type: <class 'torch.ao.nn.quantized.modules.dropout.Dropout'>\n",
      "Layer: model.fc | Type: <class 'torch.ao.nn.quantized.modules.linear.Linear'>\n",
      "Layer: model.fc._packed_params | Type: <class 'torch.ao.nn.quantized.modules.linear.LinearPackedParams'>\n"
     ]
    }
   ],
   "source": [
    "for name, module in static_quantized_model.named_modules():\n",
    "    print(f\"Layer: {name} | Type: {type(module)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "777bafd7-42a3-498c-8a58-076a1a6f5e18",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "model.lstm.layers.0.layer_fw.cell.igates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 13])\n",
      "---\n",
      "\n",
      "model.lstm.layers.0.layer_fw.cell.hgates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 24])\n",
      "---\n",
      "\n",
      "model.lstm.layers.1.layer_fw.cell.igates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 24])\n",
      "---\n",
      "\n",
      "model.lstm.layers.1.layer_fw.cell.hgates is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([96, 24])\n",
      "---\n",
      "\n",
      "model.fc is a quantized Linear layer\n",
      "Weight type: <class 'torch.Tensor'>\n",
      "Weight dtype: torch.qint8\n",
      "Weight shape: torch.Size([10, 24])\n",
      "---\n"
     ]
    }
   ],
   "source": [
    "for name, module in static_quantized_model.named_modules():\n",
    "    if isinstance(module, torch.nn.quantized.Linear):\n",
    "        print(f\"\\n{name} is a quantized Linear layer\")\n",
    "        weight = module.weight()\n",
    "        print(\"Weight type:\", type(weight))\n",
    "        print(\"Weight dtype:\", weight.dtype)\n",
    "        print(\"Weight shape:\", weight.shape)\n",
    "        print(\"---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "bf044e0d-88a6-4b63-bb63-d424ae970f79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StaticQuantizableModel(\n",
      "  (quant): Quantize(scale=tensor([3.1986]), zero_point=tensor([95]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      "  (model): LSTMClassifier(\n",
      "    (lstm): QuantizedLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): _LSTMLayer(\n",
      "          (layer_fw): _LSTMSingleLayer(\n",
      "            (cell): QuantizableLSTMCell(\n",
      "              (igates): QuantizedLinear(in_features=13, out_features=96, scale=1.1903992891311646, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "              (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.039823323488235474, zero_point=48, qscheme=torch.per_channel_affine)\n",
      "              (gates): QFunctional(\n",
      "                scale=1.1539145708084106, zero_point=63\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (input_gate): Sigmoid()\n",
      "              (forget_gate): Sigmoid()\n",
      "              (cell_gate): Tanh()\n",
      "              (output_gate): Sigmoid()\n",
      "              (fgate_cx): QFunctional(\n",
      "                scale=0.38134104013442993, zero_point=54\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (igate_cgate): QFunctional(\n",
      "                scale=0.015740342438220978, zero_point=64\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (fgate_cx_igate_cgate): QFunctional(\n",
      "                scale=0.3970814049243927, zero_point=55\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (ogate_cy): QFunctional(\n",
      "                scale=0.015732653439044952, zero_point=64\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): _LSTMLayer(\n",
      "          (layer_fw): _LSTMSingleLayer(\n",
      "            (cell): QuantizableLSTMCell(\n",
      "              (igates): QuantizedLinear(in_features=24, out_features=96, scale=0.039220649749040604, zero_point=42, qscheme=torch.per_channel_affine)\n",
      "              (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.03753151372075081, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "              (gates): QFunctional(\n",
      "                scale=0.06388549506664276, zero_point=48\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (input_gate): Sigmoid()\n",
      "              (forget_gate): Sigmoid()\n",
      "              (cell_gate): Tanh()\n",
      "              (output_gate): Sigmoid()\n",
      "              (fgate_cx): QFunctional(\n",
      "                scale=0.17570002377033234, zero_point=67\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (igate_cgate): QFunctional(\n",
      "                scale=0.01475561410188675, zero_point=65\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (fgate_cx_igate_cgate): QFunctional(\n",
      "                scale=0.1879149079322815, zero_point=67\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (ogate_cy): QFunctional(\n",
      "                scale=0.015648238360881805, zero_point=63\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): QuantizedDropout(p=0.3, inplace=False)\n",
      "    (fc): QuantizedLinear(in_features=24, out_features=10, scale=0.07591226696968079, zero_point=72, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      ")\n",
      "Quantize(scale=tensor([3.1986]), zero_point=tensor([95]), dtype=torch.quint8)\n",
      "DeQuantize()\n",
      "LSTMClassifier(\n",
      "  (lstm): QuantizedLSTM(\n",
      "    (layers): ModuleList(\n",
      "      (0): _LSTMLayer(\n",
      "        (layer_fw): _LSTMSingleLayer(\n",
      "          (cell): QuantizableLSTMCell(\n",
      "            (igates): QuantizedLinear(in_features=13, out_features=96, scale=1.1903992891311646, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "            (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.039823323488235474, zero_point=48, qscheme=torch.per_channel_affine)\n",
      "            (gates): QFunctional(\n",
      "              scale=1.1539145708084106, zero_point=63\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "            (input_gate): Sigmoid()\n",
      "            (forget_gate): Sigmoid()\n",
      "            (cell_gate): Tanh()\n",
      "            (output_gate): Sigmoid()\n",
      "            (fgate_cx): QFunctional(\n",
      "              scale=0.38134104013442993, zero_point=54\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "            (igate_cgate): QFunctional(\n",
      "              scale=0.015740342438220978, zero_point=64\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "            (fgate_cx_igate_cgate): QFunctional(\n",
      "              scale=0.3970814049243927, zero_point=55\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "            (ogate_cy): QFunctional(\n",
      "              scale=0.015732653439044952, zero_point=64\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "      (1): _LSTMLayer(\n",
      "        (layer_fw): _LSTMSingleLayer(\n",
      "          (cell): QuantizableLSTMCell(\n",
      "            (igates): QuantizedLinear(in_features=24, out_features=96, scale=0.039220649749040604, zero_point=42, qscheme=torch.per_channel_affine)\n",
      "            (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.03753151372075081, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "            (gates): QFunctional(\n",
      "              scale=0.06388549506664276, zero_point=48\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "            (input_gate): Sigmoid()\n",
      "            (forget_gate): Sigmoid()\n",
      "            (cell_gate): Tanh()\n",
      "            (output_gate): Sigmoid()\n",
      "            (fgate_cx): QFunctional(\n",
      "              scale=0.17570002377033234, zero_point=67\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "            (igate_cgate): QFunctional(\n",
      "              scale=0.01475561410188675, zero_point=65\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "            (fgate_cx_igate_cgate): QFunctional(\n",
      "              scale=0.1879149079322815, zero_point=67\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "            (ogate_cy): QFunctional(\n",
      "              scale=0.015648238360881805, zero_point=63\n",
      "              (activation_post_process): Identity()\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (dropout): QuantizedDropout(p=0.3, inplace=False)\n",
      "  (fc): QuantizedLinear(in_features=24, out_features=10, scale=0.07591226696968079, zero_point=72, qscheme=torch.per_channel_affine)\n",
      ")\n",
      "QuantizedLSTM(\n",
      "  (layers): ModuleList(\n",
      "    (0): _LSTMLayer(\n",
      "      (layer_fw): _LSTMSingleLayer(\n",
      "        (cell): QuantizableLSTMCell(\n",
      "          (igates): QuantizedLinear(in_features=13, out_features=96, scale=1.1903992891311646, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "          (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.039823323488235474, zero_point=48, qscheme=torch.per_channel_affine)\n",
      "          (gates): QFunctional(\n",
      "            scale=1.1539145708084106, zero_point=63\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "          (input_gate): Sigmoid()\n",
      "          (forget_gate): Sigmoid()\n",
      "          (cell_gate): Tanh()\n",
      "          (output_gate): Sigmoid()\n",
      "          (fgate_cx): QFunctional(\n",
      "            scale=0.38134104013442993, zero_point=54\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "          (igate_cgate): QFunctional(\n",
      "            scale=0.015740342438220978, zero_point=64\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "          (fgate_cx_igate_cgate): QFunctional(\n",
      "            scale=0.3970814049243927, zero_point=55\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "          (ogate_cy): QFunctional(\n",
      "            scale=0.015732653439044952, zero_point=64\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (1): _LSTMLayer(\n",
      "      (layer_fw): _LSTMSingleLayer(\n",
      "        (cell): QuantizableLSTMCell(\n",
      "          (igates): QuantizedLinear(in_features=24, out_features=96, scale=0.039220649749040604, zero_point=42, qscheme=torch.per_channel_affine)\n",
      "          (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.03753151372075081, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "          (gates): QFunctional(\n",
      "            scale=0.06388549506664276, zero_point=48\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "          (input_gate): Sigmoid()\n",
      "          (forget_gate): Sigmoid()\n",
      "          (cell_gate): Tanh()\n",
      "          (output_gate): Sigmoid()\n",
      "          (fgate_cx): QFunctional(\n",
      "            scale=0.17570002377033234, zero_point=67\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "          (igate_cgate): QFunctional(\n",
      "            scale=0.01475561410188675, zero_point=65\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "          (fgate_cx_igate_cgate): QFunctional(\n",
      "            scale=0.1879149079322815, zero_point=67\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "          (ogate_cy): QFunctional(\n",
      "            scale=0.015648238360881805, zero_point=63\n",
      "            (activation_post_process): Identity()\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "ModuleList(\n",
      "  (0): _LSTMLayer(\n",
      "    (layer_fw): _LSTMSingleLayer(\n",
      "      (cell): QuantizableLSTMCell(\n",
      "        (igates): QuantizedLinear(in_features=13, out_features=96, scale=1.1903992891311646, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "        (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.039823323488235474, zero_point=48, qscheme=torch.per_channel_affine)\n",
      "        (gates): QFunctional(\n",
      "          scale=1.1539145708084106, zero_point=63\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (input_gate): Sigmoid()\n",
      "        (forget_gate): Sigmoid()\n",
      "        (cell_gate): Tanh()\n",
      "        (output_gate): Sigmoid()\n",
      "        (fgate_cx): QFunctional(\n",
      "          scale=0.38134104013442993, zero_point=54\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (igate_cgate): QFunctional(\n",
      "          scale=0.015740342438220978, zero_point=64\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (fgate_cx_igate_cgate): QFunctional(\n",
      "          scale=0.3970814049243927, zero_point=55\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (ogate_cy): QFunctional(\n",
      "          scale=0.015732653439044952, zero_point=64\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      "  (1): _LSTMLayer(\n",
      "    (layer_fw): _LSTMSingleLayer(\n",
      "      (cell): QuantizableLSTMCell(\n",
      "        (igates): QuantizedLinear(in_features=24, out_features=96, scale=0.039220649749040604, zero_point=42, qscheme=torch.per_channel_affine)\n",
      "        (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.03753151372075081, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "        (gates): QFunctional(\n",
      "          scale=0.06388549506664276, zero_point=48\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (input_gate): Sigmoid()\n",
      "        (forget_gate): Sigmoid()\n",
      "        (cell_gate): Tanh()\n",
      "        (output_gate): Sigmoid()\n",
      "        (fgate_cx): QFunctional(\n",
      "          scale=0.17570002377033234, zero_point=67\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (igate_cgate): QFunctional(\n",
      "          scale=0.01475561410188675, zero_point=65\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (fgate_cx_igate_cgate): QFunctional(\n",
      "          scale=0.1879149079322815, zero_point=67\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "        (ogate_cy): QFunctional(\n",
      "          scale=0.015648238360881805, zero_point=63\n",
      "          (activation_post_process): Identity()\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "_LSTMLayer(\n",
      "  (layer_fw): _LSTMSingleLayer(\n",
      "    (cell): QuantizableLSTMCell(\n",
      "      (igates): QuantizedLinear(in_features=13, out_features=96, scale=1.1903992891311646, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "      (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.039823323488235474, zero_point=48, qscheme=torch.per_channel_affine)\n",
      "      (gates): QFunctional(\n",
      "        scale=1.1539145708084106, zero_point=63\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (input_gate): Sigmoid()\n",
      "      (forget_gate): Sigmoid()\n",
      "      (cell_gate): Tanh()\n",
      "      (output_gate): Sigmoid()\n",
      "      (fgate_cx): QFunctional(\n",
      "        scale=0.38134104013442993, zero_point=54\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (igate_cgate): QFunctional(\n",
      "        scale=0.015740342438220978, zero_point=64\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (fgate_cx_igate_cgate): QFunctional(\n",
      "        scale=0.3970814049243927, zero_point=55\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (ogate_cy): QFunctional(\n",
      "        scale=0.015732653439044952, zero_point=64\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "_LSTMSingleLayer(\n",
      "  (cell): QuantizableLSTMCell(\n",
      "    (igates): QuantizedLinear(in_features=13, out_features=96, scale=1.1903992891311646, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "    (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.039823323488235474, zero_point=48, qscheme=torch.per_channel_affine)\n",
      "    (gates): QFunctional(\n",
      "      scale=1.1539145708084106, zero_point=63\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "    (input_gate): Sigmoid()\n",
      "    (forget_gate): Sigmoid()\n",
      "    (cell_gate): Tanh()\n",
      "    (output_gate): Sigmoid()\n",
      "    (fgate_cx): QFunctional(\n",
      "      scale=0.38134104013442993, zero_point=54\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "    (igate_cgate): QFunctional(\n",
      "      scale=0.015740342438220978, zero_point=64\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "    (fgate_cx_igate_cgate): QFunctional(\n",
      "      scale=0.3970814049243927, zero_point=55\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "    (ogate_cy): QFunctional(\n",
      "      scale=0.015732653439044952, zero_point=64\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "QuantizableLSTMCell(\n",
      "  (igates): QuantizedLinear(in_features=13, out_features=96, scale=1.1903992891311646, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "  (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.039823323488235474, zero_point=48, qscheme=torch.per_channel_affine)\n",
      "  (gates): QFunctional(\n",
      "    scale=1.1539145708084106, zero_point=63\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      "  (input_gate): Sigmoid()\n",
      "  (forget_gate): Sigmoid()\n",
      "  (cell_gate): Tanh()\n",
      "  (output_gate): Sigmoid()\n",
      "  (fgate_cx): QFunctional(\n",
      "    scale=0.38134104013442993, zero_point=54\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      "  (igate_cgate): QFunctional(\n",
      "    scale=0.015740342438220978, zero_point=64\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      "  (fgate_cx_igate_cgate): QFunctional(\n",
      "    scale=0.3970814049243927, zero_point=55\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      "  (ogate_cy): QFunctional(\n",
      "    scale=0.015732653439044952, zero_point=64\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      ")\n",
      "QuantizedLinear(in_features=13, out_features=96, scale=1.1903992891311646, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "(tensor([[-0.0252,  0.1735,  0.1707,  ..., -0.0868,  0.1777, -0.0728],\n",
      "        [-0.0712, -0.0037, -0.0055,  ...,  0.1132,  0.0091, -0.1918],\n",
      "        [-0.0114, -0.1428, -0.2438,  ...,  0.0057, -0.0705, -0.1562],\n",
      "        ...,\n",
      "        [-0.1248, -0.1339, -0.0850,  ...,  0.0507,  0.1085,  0.1501],\n",
      "        [ 0.0761,  0.1331,  0.0285,  ...,  0.1122,  0.0970,  0.0114],\n",
      "        [ 0.0356,  0.0613,  0.0099,  ...,  0.1700, -0.1146, -0.1779]],\n",
      "       size=(96, 13), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0014, 0.0018, 0.0019, 0.0019, 0.0021, 0.0016, 0.0017, 0.0018, 0.0018,\n",
      "        0.0015, 0.0015, 0.0018, 0.0015, 0.0015, 0.0021, 0.0015, 0.0021, 0.0022,\n",
      "        0.0015, 0.0022, 0.0021, 0.0023, 0.0010, 0.0015, 0.0015, 0.0013, 0.0016,\n",
      "        0.0018, 0.0014, 0.0015, 0.0017, 0.0021, 0.0017, 0.0016, 0.0018, 0.0014,\n",
      "        0.0021, 0.0022, 0.0017, 0.0015, 0.0017, 0.0016, 0.0017, 0.0018, 0.0024,\n",
      "        0.0018, 0.0015, 0.0016, 0.0011, 0.0017, 0.0016, 0.0022, 0.0016, 0.0018,\n",
      "        0.0015, 0.0018, 0.0017, 0.0020, 0.0020, 0.0022, 0.0018, 0.0021, 0.0018,\n",
      "        0.0018, 0.0017, 0.0015, 0.0019, 0.0021, 0.0020, 0.0019, 0.0018, 0.0021,\n",
      "        0.0021, 0.0024, 0.0019, 0.0019, 0.0019, 0.0019, 0.0016, 0.0021, 0.0022,\n",
      "        0.0017, 0.0017, 0.0015, 0.0013, 0.0018, 0.0022, 0.0016, 0.0018, 0.0022,\n",
      "        0.0018, 0.0013, 0.0017, 0.0018, 0.0019, 0.0020], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0), Parameter containing:\n",
      "tensor([ 0.2673,  0.0389,  0.2075,  0.1181,  0.2249,  0.1796,  0.1552, -0.0217,\n",
      "         0.2757, -0.0083,  0.2877,  0.0659,  0.0650,  0.1301, -0.0784,  0.1830,\n",
      "         0.1753, -0.1754, -0.0572, -0.0480,  0.2117, -0.0848,  0.1219,  0.1269,\n",
      "         0.0858, -0.0577,  0.1393,  0.0821,  0.2179,  0.0153,  0.1729,  0.0088,\n",
      "         0.3024,  0.1284,  0.0527,  0.0096,  0.2988,  0.1410,  0.2889,  0.1769,\n",
      "         0.0962,  0.1555,  0.0157,  0.1555,  0.0538, -0.0921,  0.3090,  0.0400,\n",
      "         0.1801,  0.0676,  0.2005, -0.0830,  0.2032,  0.1322,  0.1874, -0.1320,\n",
      "         0.1858, -0.1975, -0.0690, -0.0743,  0.1408, -0.2532,  0.0645,  0.0492,\n",
      "        -0.0873, -0.0128,  0.0107,  0.1541, -0.0935,  0.1669,  0.2163,  0.1071,\n",
      "         0.1498,  0.0393,  0.1925,  0.2969,  0.1369,  0.0514,  0.2519,  0.2220,\n",
      "         0.2239,  0.0493,  0.0704,  0.2629,  0.2313, -0.0571,  0.2158,  0.1750,\n",
      "         0.0635,  0.1823,  0.1307,  0.0309, -0.0238,  0.1785,  0.3233,  0.1078],\n",
      "       requires_grad=True))\n",
      "QuantizedLinear(in_features=24, out_features=96, scale=0.039823323488235474, zero_point=48, qscheme=torch.per_channel_affine)\n",
      "(tensor([[ 0.0292,  0.0646, -0.1125,  ...,  0.1479, -0.1083,  0.0208],\n",
      "        [-0.0532,  0.1233, -0.0870,  ...,  0.1886,  0.1233, -0.0290],\n",
      "        [ 0.0055,  0.0074,  0.0827,  ...,  0.1746, -0.1930, -0.0165],\n",
      "        ...,\n",
      "        [ 0.1395,  0.0336,  0.0159,  ...,  0.1837, -0.1678,  0.1748],\n",
      "        [ 0.1977, -0.0400,  0.0000,  ..., -0.0250, -0.3179, -0.2127],\n",
      "        [-0.0670,  0.0906,  0.0177,  ..., -0.1556, -0.0611,  0.0827]],\n",
      "       size=(96, 24), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0021, 0.0024, 0.0018, 0.0024, 0.0027, 0.0019, 0.0023, 0.0020, 0.0023,\n",
      "        0.0017, 0.0030, 0.0016, 0.0025, 0.0026, 0.0022, 0.0018, 0.0019, 0.0020,\n",
      "        0.0018, 0.0018, 0.0022, 0.0025, 0.0022, 0.0019, 0.0024, 0.0020, 0.0024,\n",
      "        0.0020, 0.0022, 0.0027, 0.0026, 0.0021, 0.0024, 0.0025, 0.0024, 0.0023,\n",
      "        0.0025, 0.0021, 0.0024, 0.0022, 0.0024, 0.0017, 0.0024, 0.0022, 0.0021,\n",
      "        0.0021, 0.0021, 0.0020, 0.0020, 0.0021, 0.0022, 0.0027, 0.0026, 0.0021,\n",
      "        0.0017, 0.0020, 0.0024, 0.0023, 0.0031, 0.0018, 0.0021, 0.0025, 0.0025,\n",
      "        0.0017, 0.0023, 0.0016, 0.0028, 0.0025, 0.0019, 0.0019, 0.0021, 0.0019,\n",
      "        0.0030, 0.0021, 0.0026, 0.0025, 0.0029, 0.0025, 0.0021, 0.0023, 0.0023,\n",
      "        0.0023, 0.0024, 0.0022, 0.0024, 0.0028, 0.0022, 0.0020, 0.0025, 0.0026,\n",
      "        0.0024, 0.0025, 0.0026, 0.0018, 0.0025, 0.0020], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0), Parameter containing:\n",
      "tensor([ 0.2388, -0.0583,  0.1456,  0.3237,  0.0819,  0.0123, -0.0540, -0.1220,\n",
      "         0.0267, -0.1209,  0.2498,  0.0714,  0.0268,  0.1218,  0.1310,  0.1291,\n",
      "         0.2593, -0.1918,  0.2264,  0.0731, -0.0486,  0.0644,  0.1437,  0.0336,\n",
      "         0.0734, -0.0074, -0.0404,  0.3362, -0.0236, -0.0091,  0.2382, -0.0616,\n",
      "        -0.0502,  0.1439,  0.1587,  0.2052,  0.2890,  0.2282,  0.1414,  0.1027,\n",
      "         0.0769,  0.0960,  0.0432,  0.2053,  0.0913, -0.1184, -0.0428, -0.0749,\n",
      "         0.2431,  0.1096,  0.1551,  0.0712, -0.0021,  0.2422, -0.0825, -0.1557,\n",
      "         0.0750, -0.1792, -0.0473,  0.1745, -0.1260, -0.2401, -0.1902,  0.1834,\n",
      "        -0.1251,  0.1328, -0.1485,  0.1136, -0.1533,  0.0560, -0.1459, -0.0470,\n",
      "         0.1543, -0.1254,  0.2600,  0.2846,  0.2972,  0.0651, -0.0653,  0.1608,\n",
      "         0.1434,  0.2546,  0.3021,  0.2029,  0.0291,  0.1467,  0.1118,  0.0486,\n",
      "         0.3158,  0.0608,  0.1314, -0.1211,  0.1932,  0.1143,  0.3227,  0.1996],\n",
      "       requires_grad=True))\n",
      "QFunctional(\n",
      "  scale=1.1539145708084106, zero_point=63\n",
      "  (activation_post_process): Identity()\n",
      ")\n",
      "Identity()\n",
      "Sigmoid()\n",
      "Sigmoid()\n",
      "Tanh()\n",
      "Sigmoid()\n",
      "QFunctional(\n",
      "  scale=0.38134104013442993, zero_point=54\n",
      "  (activation_post_process): Identity()\n",
      ")\n",
      "Identity()\n",
      "QFunctional(\n",
      "  scale=0.015740342438220978, zero_point=64\n",
      "  (activation_post_process): Identity()\n",
      ")\n",
      "Identity()\n",
      "QFunctional(\n",
      "  scale=0.3970814049243927, zero_point=55\n",
      "  (activation_post_process): Identity()\n",
      ")\n",
      "Identity()\n",
      "QFunctional(\n",
      "  scale=0.015732653439044952, zero_point=64\n",
      "  (activation_post_process): Identity()\n",
      ")\n",
      "Identity()\n",
      "_LSTMLayer(\n",
      "  (layer_fw): _LSTMSingleLayer(\n",
      "    (cell): QuantizableLSTMCell(\n",
      "      (igates): QuantizedLinear(in_features=24, out_features=96, scale=0.039220649749040604, zero_point=42, qscheme=torch.per_channel_affine)\n",
      "      (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.03753151372075081, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "      (gates): QFunctional(\n",
      "        scale=0.06388549506664276, zero_point=48\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (input_gate): Sigmoid()\n",
      "      (forget_gate): Sigmoid()\n",
      "      (cell_gate): Tanh()\n",
      "      (output_gate): Sigmoid()\n",
      "      (fgate_cx): QFunctional(\n",
      "        scale=0.17570002377033234, zero_point=67\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (igate_cgate): QFunctional(\n",
      "        scale=0.01475561410188675, zero_point=65\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (fgate_cx_igate_cgate): QFunctional(\n",
      "        scale=0.1879149079322815, zero_point=67\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "      (ogate_cy): QFunctional(\n",
      "        scale=0.015648238360881805, zero_point=63\n",
      "        (activation_post_process): Identity()\n",
      "      )\n",
      "    )\n",
      "  )\n",
      ")\n",
      "_LSTMSingleLayer(\n",
      "  (cell): QuantizableLSTMCell(\n",
      "    (igates): QuantizedLinear(in_features=24, out_features=96, scale=0.039220649749040604, zero_point=42, qscheme=torch.per_channel_affine)\n",
      "    (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.03753151372075081, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "    (gates): QFunctional(\n",
      "      scale=0.06388549506664276, zero_point=48\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "    (input_gate): Sigmoid()\n",
      "    (forget_gate): Sigmoid()\n",
      "    (cell_gate): Tanh()\n",
      "    (output_gate): Sigmoid()\n",
      "    (fgate_cx): QFunctional(\n",
      "      scale=0.17570002377033234, zero_point=67\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "    (igate_cgate): QFunctional(\n",
      "      scale=0.01475561410188675, zero_point=65\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "    (fgate_cx_igate_cgate): QFunctional(\n",
      "      scale=0.1879149079322815, zero_point=67\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "    (ogate_cy): QFunctional(\n",
      "      scale=0.015648238360881805, zero_point=63\n",
      "      (activation_post_process): Identity()\n",
      "    )\n",
      "  )\n",
      ")\n",
      "QuantizableLSTMCell(\n",
      "  (igates): QuantizedLinear(in_features=24, out_features=96, scale=0.039220649749040604, zero_point=42, qscheme=torch.per_channel_affine)\n",
      "  (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.03753151372075081, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "  (gates): QFunctional(\n",
      "    scale=0.06388549506664276, zero_point=48\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      "  (input_gate): Sigmoid()\n",
      "  (forget_gate): Sigmoid()\n",
      "  (cell_gate): Tanh()\n",
      "  (output_gate): Sigmoid()\n",
      "  (fgate_cx): QFunctional(\n",
      "    scale=0.17570002377033234, zero_point=67\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      "  (igate_cgate): QFunctional(\n",
      "    scale=0.01475561410188675, zero_point=65\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      "  (fgate_cx_igate_cgate): QFunctional(\n",
      "    scale=0.1879149079322815, zero_point=67\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      "  (ogate_cy): QFunctional(\n",
      "    scale=0.015648238360881805, zero_point=63\n",
      "    (activation_post_process): Identity()\n",
      "  )\n",
      ")\n",
      "QuantizedLinear(in_features=24, out_features=96, scale=0.039220649749040604, zero_point=42, qscheme=torch.per_channel_affine)\n",
      "(tensor([[ 0.1607,  0.0024, -0.0118,  ...,  0.1370, -0.2457,  0.0496],\n",
      "        [ 0.2573,  0.0588, -0.1103,  ...,  0.0123, -0.3137,  0.0270],\n",
      "        [-0.1134,  0.1276,  0.1607,  ..., -0.2245, -0.0284,  0.1040],\n",
      "        ...,\n",
      "        [-0.0517,  0.2009,  0.2446,  ..., -0.1173,  0.0696, -0.1869],\n",
      "        [ 0.3457,  0.2153,  0.4142,  ...,  0.1892,  0.1924, -0.2185],\n",
      "        [ 0.1028,  0.0881,  0.3193,  ...,  0.1211, -0.1725, -0.1321]],\n",
      "       size=(96, 24), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0024, 0.0025, 0.0024, 0.0021, 0.0019, 0.0017, 0.0023, 0.0022, 0.0023,\n",
      "        0.0019, 0.0019, 0.0023, 0.0015, 0.0026, 0.0020, 0.0022, 0.0018, 0.0020,\n",
      "        0.0024, 0.0017, 0.0020, 0.0022, 0.0025, 0.0022, 0.0022, 0.0024, 0.0021,\n",
      "        0.0027, 0.0025, 0.0023, 0.0016, 0.0028, 0.0030, 0.0022, 0.0025, 0.0023,\n",
      "        0.0018, 0.0028, 0.0028, 0.0023, 0.0023, 0.0026, 0.0025, 0.0028, 0.0024,\n",
      "        0.0023, 0.0027, 0.0022, 0.0023, 0.0020, 0.0028, 0.0020, 0.0017, 0.0020,\n",
      "        0.0021, 0.0019, 0.0022, 0.0021, 0.0020, 0.0019, 0.0017, 0.0020, 0.0023,\n",
      "        0.0021, 0.0021, 0.0025, 0.0022, 0.0016, 0.0022, 0.0022, 0.0027, 0.0023,\n",
      "        0.0028, 0.0028, 0.0026, 0.0023, 0.0023, 0.0023, 0.0024, 0.0027, 0.0024,\n",
      "        0.0024, 0.0024, 0.0024, 0.0021, 0.0034, 0.0027, 0.0032, 0.0028, 0.0028,\n",
      "        0.0027, 0.0029, 0.0015, 0.0020, 0.0033, 0.0037], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0), Parameter containing:\n",
      "tensor([ 0.3626,  0.0047,  0.1873,  0.1865, -0.0095,  0.1285,  0.0121,  0.1768,\n",
      "         0.0341, -0.1582,  0.0941, -0.1196, -0.0347,  0.1568,  0.1054,  0.1856,\n",
      "         0.0928,  0.0404,  0.3064, -0.0269,  0.2452,  0.2064,  0.1176,  0.0023,\n",
      "         0.0760,  0.0737, -0.0054,  0.3152,  0.0470,  0.1483, -0.0364, -0.0678,\n",
      "         0.2296,  0.0525,  0.2980,  0.2182, -0.1100,  0.2443, -0.0353,  0.0830,\n",
      "        -0.1146,  0.3046,  0.0937,  0.2593,  0.0581,  0.2176,  0.0625,  0.3025,\n",
      "        -0.0863, -0.1703, -0.0729,  0.0885,  0.1132, -0.1935,  0.0709, -0.1498,\n",
      "        -0.1149, -0.0914,  0.1584, -0.0244, -0.0406, -0.2109,  0.2255, -0.0567,\n",
      "         0.0652, -0.0956,  0.0105, -0.2182,  0.0584,  0.1067,  0.0666,  0.0125,\n",
      "         0.0423, -0.0275,  0.2634,  0.2472,  0.0973,  0.0603,  0.0947, -0.0254,\n",
      "         0.1688, -0.1194,  0.2475,  0.0833, -0.0283,  0.0828,  0.2325,  0.1246,\n",
      "        -0.0199, -0.0050,  0.2079,  0.0008,  0.2093, -0.0167,  0.1604,  0.2106],\n",
      "       requires_grad=True))\n",
      "QuantizedLinear(in_features=24, out_features=96, scale=0.03753151372075081, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "(tensor([[-0.1566,  0.0598,  0.1584,  ...,  0.1901,  0.1038,  0.1778],\n",
      "        [ 0.1633, -0.0268,  0.1096,  ..., -0.0738,  0.1163, -0.0291],\n",
      "        [ 0.0751, -0.0789,  0.1887,  ..., -0.1848, -0.1868,  0.1945],\n",
      "        ...,\n",
      "        [ 0.1680,  0.2830, -0.0644,  ...,  0.2163,  0.2738, -0.0805],\n",
      "        [-0.0771,  0.0345,  0.0159,  ...,  0.2073, -0.0239, -0.0771],\n",
      "        [-0.0949, -0.2373,  0.2452,  ...,  0.0026,  0.0633,  0.0343]],\n",
      "       size=(96, 24), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0018, 0.0022, 0.0019, 0.0019, 0.0020, 0.0018, 0.0021, 0.0019, 0.0020,\n",
      "        0.0019, 0.0020, 0.0019, 0.0024, 0.0020, 0.0023, 0.0028, 0.0021, 0.0024,\n",
      "        0.0026, 0.0020, 0.0015, 0.0021, 0.0020, 0.0022, 0.0021, 0.0020, 0.0016,\n",
      "        0.0020, 0.0023, 0.0022, 0.0023, 0.0018, 0.0025, 0.0016, 0.0023, 0.0022,\n",
      "        0.0022, 0.0021, 0.0018, 0.0024, 0.0018, 0.0029, 0.0024, 0.0026, 0.0023,\n",
      "        0.0023, 0.0017, 0.0025, 0.0025, 0.0019, 0.0016, 0.0022, 0.0015, 0.0019,\n",
      "        0.0017, 0.0026, 0.0023, 0.0021, 0.0019, 0.0020, 0.0020, 0.0024, 0.0015,\n",
      "        0.0024, 0.0021, 0.0020, 0.0021, 0.0017, 0.0018, 0.0026, 0.0025, 0.0024,\n",
      "        0.0021, 0.0022, 0.0018, 0.0019, 0.0018, 0.0022, 0.0023, 0.0016, 0.0021,\n",
      "        0.0023, 0.0022, 0.0024, 0.0023, 0.0027, 0.0021, 0.0026, 0.0024, 0.0031,\n",
      "        0.0029, 0.0022, 0.0021, 0.0023, 0.0027, 0.0026], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
      "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]),\n",
      "       axis=0), Parameter containing:\n",
      "tensor([ 0.0026, -0.0124, -0.0451,  0.2540,  0.1576, -0.1054,  0.1881,  0.0154,\n",
      "         0.1170,  0.2378,  0.2183,  0.0918,  0.0037, -0.0125, -0.1234,  0.0805,\n",
      "         0.0537,  0.0266,  0.0622,  0.0569, -0.0815, -0.0236,  0.0615, -0.0785,\n",
      "         0.1165,  0.1414, -0.0889,  0.0493, -0.1347,  0.1633,  0.0241,  0.2504,\n",
      "         0.2806,  0.1360,  0.2686,  0.0750, -0.0911,  0.0540, -0.0274,  0.0706,\n",
      "         0.0756, -0.0267,  0.2881, -0.0534,  0.0225,  0.0648,  0.0197,  0.1211,\n",
      "        -0.0805,  0.1739, -0.0516,  0.0590,  0.0786, -0.0993,  0.1174, -0.0331,\n",
      "         0.0660,  0.1372,  0.1310, -0.0712, -0.0416,  0.1085,  0.0696, -0.0629,\n",
      "         0.1239,  0.1931,  0.0832, -0.0006,  0.1162,  0.0693, -0.2309,  0.1224,\n",
      "         0.3159, -0.0279,  0.2527,  0.2810, -0.0312,  0.2375,  0.2464,  0.0520,\n",
      "         0.2490,  0.0687,  0.3133,  0.2161,  0.2108, -0.0056,  0.0420,  0.3577,\n",
      "         0.0526,  0.0135,  0.3407,  0.0008, -0.0144,  0.0598,  0.1309,  0.0478],\n",
      "       requires_grad=True))\n",
      "QFunctional(\n",
      "  scale=0.06388549506664276, zero_point=48\n",
      "  (activation_post_process): Identity()\n",
      ")\n",
      "Identity()\n",
      "Sigmoid()\n",
      "Sigmoid()\n",
      "Tanh()\n",
      "Sigmoid()\n",
      "QFunctional(\n",
      "  scale=0.17570002377033234, zero_point=67\n",
      "  (activation_post_process): Identity()\n",
      ")\n",
      "Identity()\n",
      "QFunctional(\n",
      "  scale=0.01475561410188675, zero_point=65\n",
      "  (activation_post_process): Identity()\n",
      ")\n",
      "Identity()\n",
      "QFunctional(\n",
      "  scale=0.1879149079322815, zero_point=67\n",
      "  (activation_post_process): Identity()\n",
      ")\n",
      "Identity()\n",
      "QFunctional(\n",
      "  scale=0.015648238360881805, zero_point=63\n",
      "  (activation_post_process): Identity()\n",
      ")\n",
      "Identity()\n",
      "QuantizedDropout(p=0.3, inplace=False)\n",
      "QuantizedLinear(in_features=24, out_features=10, scale=0.07591226696968079, zero_point=72, qscheme=torch.per_channel_affine)\n",
      "(tensor([[ 0.2979,  0.3596, -0.4007, -0.3596, -0.0274,  0.0514, -0.4383, -0.0753,\n",
      "         -0.0103,  0.0000, -0.1404,  0.0342, -0.1438, -0.2157, -0.0342, -0.1233,\n",
      "          0.2397, -0.3630,  0.0000, -0.1746, -0.2431,  0.1370,  0.2979,  0.0240],\n",
      "        [ 0.1173, -0.0733,  0.0049, -0.0733, -0.0244,  0.2566, -0.1515,  0.0953,\n",
      "         -0.0782, -0.2468,  0.0024, -0.1002, -0.1368,  0.2468, -0.0880,  0.0269,\n",
      "         -0.3006, -0.2322, -0.2688,  0.2493, -0.3128, -0.1613, -0.1711, -0.0489],\n",
      "        [ 0.0530,  0.0505,  0.1288,  0.1313, -0.0884,  0.2045,  0.1313, -0.0227,\n",
      "         -0.0909,  0.2272, -0.2878, -0.2550,  0.0909, -0.1262,  0.1414, -0.2601,\n",
      "         -0.0126, -0.1060, -0.0707, -0.3232, -0.1692,  0.0884,  0.1717, -0.1111],\n",
      "        [ 0.1701,  0.1230, -0.0579,  0.0289,  0.1628, -0.4052,  0.0181, -0.0977,\n",
      "         -0.0760,  0.2207, -0.1375,  0.3112, -0.0977, -0.2135,  0.0326, -0.0145,\n",
      "          0.2424,  0.4595,  0.3546,  0.0072,  0.2605, -0.3799,  0.2243,  0.1375],\n",
      "        [ 0.0546, -0.0115, -0.1093,  0.1668,  0.1179, -0.1323,  0.3623, -0.1006,\n",
      "         -0.0575,  0.1955, -0.2703,  0.0575,  0.1524, -0.3680, -0.0661, -0.3249,\n",
      "         -0.0374, -0.1696,  0.2128,  0.0460, -0.0259,  0.2013,  0.3594, -0.1696],\n",
      "        [-0.1885, -0.2372,  0.3406,  0.2372, -0.2037, -0.0943, -0.1977, -0.1460,\n",
      "          0.0243, -0.0030,  0.2858, -0.0395,  0.0608,  0.3497,  0.1490,  0.3102,\n",
      "         -0.2129, -0.1216, -0.1247,  0.1277, -0.3193,  0.1916, -0.1490,  0.3862],\n",
      "        [-0.4127,  0.1477, -0.1060,  0.3900,  0.1401, -0.4846, -0.0265,  0.3862,\n",
      "          0.2272, -0.1022,  0.1855,  0.0454,  0.3748,  0.2840,  0.1401, -0.1628,\n",
      "          0.0189,  0.3294, -0.1628, -0.3748,  0.1060, -0.3143, -0.3938,  0.3975],\n",
      "        [-0.0670, -0.2143, -0.0938, -0.1849, -0.1715,  0.1902, -0.0831, -0.2036,\n",
      "          0.0161, -0.3402, -0.0241, -0.2277, -0.2116,  0.1500,  0.0938,  0.2867,\n",
      "         -0.2438,  0.0804, -0.1393,  0.3135,  0.0295,  0.0536, -0.2116, -0.2197],\n",
      "        [-0.2159,  0.0655, -0.2892,  0.1928, -0.2043, -0.1157,  0.3432,  0.4897,\n",
      "          0.4126, -0.0540,  0.2622,  0.3354,  0.1002, -0.2699, -0.1427, -0.3624,\n",
      "          0.2198,  0.4010,  0.4357, -0.1388,  0.2275, -0.2352, -0.4357,  0.3624],\n",
      "        [ 0.2859, -0.2387,  0.0473, -0.0248,  0.1351,  0.2004, -0.2004, -0.1216,\n",
      "         -0.0113,  0.0090, -0.1531,  0.0180,  0.0428,  0.2296, -0.0293,  0.2859,\n",
      "         -0.1981, -0.0315, -0.2522,  0.2634, -0.1418,  0.0135,  0.0630, -0.1058]],\n",
      "       size=(10, 24), dtype=torch.qint8,\n",
      "       quantization_scheme=torch.per_channel_affine,\n",
      "       scale=tensor([0.0034, 0.0024, 0.0025, 0.0036, 0.0029, 0.0030, 0.0038, 0.0027, 0.0039,\n",
      "        0.0023], dtype=torch.float64),\n",
      "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0]), axis=0), Parameter containing:\n",
      "tensor([-0.1831,  0.0804, -0.1522,  0.0301, -0.1607,  0.1601,  0.0631,  0.0037,\n",
      "         0.2228, -0.1806], requires_grad=True))\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "for name, module in static_quantized_model.named_modules():\n",
    "    print(module)\n",
    "    if hasattr(module, 'weight') and isinstance(module.weight, torch.Tensor):\n",
    "        weight_tensor = module.weight()\n",
    "        if weight_tensor.is_quantized:\n",
    "            byte_size = weight_tensor.int_repr().element_size() * weight_tensor.int_repr().nelement()\n",
    "            print(f\"{name} quantized weight size: {byte_size / 1024:.4f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "215a22e9-cbcb-4d87-bd7b-6e61ab801874",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Estimated FP32 size: 32.81 KB\n",
      "Estimated INT8 size: 8.20 KB\n",
      "Compression ratio: 4.00x\n"
     ]
    }
   ],
   "source": [
    "total_fp32_bytes = 0\n",
    "total_int8_bytes = 0\n",
    "\n",
    "for name, module in static_quantized_model.named_modules():\n",
    "    if isinstance(module, torch.nn.quantized.Linear):\n",
    "        weight = module.weight()\n",
    "        if weight.is_quantized:\n",
    "            num_elements = weight.numel()\n",
    "            total_fp32_bytes += num_elements * 4  # FP32\n",
    "            total_int8_bytes += num_elements * 1  # INT8\n",
    "\n",
    "print(f\"Estimated FP32 size: {total_fp32_bytes / 1024:.2f} KB\")\n",
    "print(f\"Estimated INT8 size: {total_int8_bytes / 1024:.2f} KB\")\n",
    "print(f\"Compression ratio: {total_fp32_bytes / total_int8_bytes:.2f}x\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "09ca2fb5-ffd4-4514-9649-71d08c81768f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "StaticQuantizableModel(\n",
      "  (quant): Quantize(scale=tensor([3.1986]), zero_point=tensor([95]), dtype=torch.quint8)\n",
      "  (dequant): DeQuantize()\n",
      "  (model): LSTMClassifier(\n",
      "    (lstm): QuantizedLSTM(\n",
      "      (layers): ModuleList(\n",
      "        (0): _LSTMLayer(\n",
      "          (layer_fw): _LSTMSingleLayer(\n",
      "            (cell): QuantizableLSTMCell(\n",
      "              (igates): QuantizedLinear(in_features=13, out_features=96, scale=1.1903992891311646, zero_point=63, qscheme=torch.per_channel_affine)\n",
      "              (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.039823323488235474, zero_point=48, qscheme=torch.per_channel_affine)\n",
      "              (gates): QFunctional(\n",
      "                scale=1.1539145708084106, zero_point=63\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (input_gate): Sigmoid()\n",
      "              (forget_gate): Sigmoid()\n",
      "              (cell_gate): Tanh()\n",
      "              (output_gate): Sigmoid()\n",
      "              (fgate_cx): QFunctional(\n",
      "                scale=0.38134104013442993, zero_point=54\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (igate_cgate): QFunctional(\n",
      "                scale=0.015740342438220978, zero_point=64\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (fgate_cx_igate_cgate): QFunctional(\n",
      "                scale=0.3970814049243927, zero_point=55\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (ogate_cy): QFunctional(\n",
      "                scale=0.015732653439044952, zero_point=64\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "        (1): _LSTMLayer(\n",
      "          (layer_fw): _LSTMSingleLayer(\n",
      "            (cell): QuantizableLSTMCell(\n",
      "              (igates): QuantizedLinear(in_features=24, out_features=96, scale=0.039220649749040604, zero_point=42, qscheme=torch.per_channel_affine)\n",
      "              (hgates): QuantizedLinear(in_features=24, out_features=96, scale=0.03753151372075081, zero_point=53, qscheme=torch.per_channel_affine)\n",
      "              (gates): QFunctional(\n",
      "                scale=0.06388549506664276, zero_point=48\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (input_gate): Sigmoid()\n",
      "              (forget_gate): Sigmoid()\n",
      "              (cell_gate): Tanh()\n",
      "              (output_gate): Sigmoid()\n",
      "              (fgate_cx): QFunctional(\n",
      "                scale=0.17570002377033234, zero_point=67\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (igate_cgate): QFunctional(\n",
      "                scale=0.01475561410188675, zero_point=65\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (fgate_cx_igate_cgate): QFunctional(\n",
      "                scale=0.1879149079322815, zero_point=67\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "              (ogate_cy): QFunctional(\n",
      "                scale=0.015648238360881805, zero_point=63\n",
      "                (activation_post_process): Identity()\n",
      "              )\n",
      "            )\n",
      "          )\n",
      "        )\n",
      "      )\n",
      "    )\n",
      "    (dropout): QuantizedDropout(p=0.3, inplace=False)\n",
      "    (fc): QuantizedLinear(in_features=24, out_features=10, scale=0.07591226696968079, zero_point=72, qscheme=torch.per_channel_affine)\n",
      "  )\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(static_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "5713a6fe-fe6e-4b79-8bb7-46631eaeb625",
   "metadata": {},
   "outputs": [],
   "source": [
    "def round_scale_to_power_of_two(scale):\n",
    "    return float(2 ** round(np.log2(scale)))\n",
    "\n",
    "# Function to update quantized weights with new scale\n",
    "def update_quantized_weights(module: nn.Module):\n",
    "    for name, submodule in module.named_children():\n",
    "        if isinstance(submodule, torch.nn.quantized.Linear):\n",
    "            old_wt = submodule.weight()\n",
    "            old_scale = old_wt.q_scale()\n",
    "            old_zp = old_wt.q_zero_point()\n",
    "\n",
    "            # Dequantize\n",
    "            float_wt = old_wt.dequantize()\n",
    "\n",
    "            # Round scale to nearest power of 2\n",
    "            new_scale = round_scale_to_power_of_two(old_scale)\n",
    "\n",
    "            # Requantize with new scale\n",
    "            new_qweight = torch.quantize_per_tensor(float_wt, scale=new_scale, zero_point=old_zp, dtype=torch.qint8)\n",
    "\n",
    "            # Replace weight\n",
    "            submodule.set_weight_bias(new_qweight, submodule.bias())\n",
    "\n",
    "            print(f\"[Updated] {name}: scale {old_scale:.6f} → {new_scale:.6f}, zero_point = {old_zp}\")\n",
    "\n",
    "        else:\n",
    "            update_quantized_weights(submodule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "324e7ce5-dcaa-48f4-a756-c86f53f3d7b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🔍 Updating quantized weights to use power-of-2 scales...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'static_quantized_model' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# --- Step 1: Extract and update all quantized scales in-place ---\u001b[39;00m\n\u001b[1;32m      2\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m🔍 Updating quantized weights to use power-of-2 scales...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m----> 3\u001b[0m update_quantized_weights(\u001b[43mstatic_quantized_model\u001b[49m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'static_quantized_model' is not defined"
     ]
    }
   ],
   "source": [
    "# --- Step 1: Extract and update all quantized scales in-place ---\n",
    "print(\"🔍 Updating quantized weights to use power-of-2 scales...\")\n",
    "update_quantized_weights(static_quantized_model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8cd6a3d-2813-47f7-8c02-f241f51578ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.evaluate import ModelEvaluator\n",
    "\n",
    "\n",
    "static_quantized_model.eval()\n",
    "device = torch.device(\"cpu\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "static_quantized_model.to(device)\n",
    "\n",
    "\n",
    "quant_test_instance_2 = ModelEvaluator(\n",
    "    static_quantized_model, \n",
    "    test_loader,\n",
    "    device\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bc0f5b1-a0c3-475a-b3f4-e1da0a1eef51",
   "metadata": {},
   "outputs": [],
   "source": [
    "quant_test_instance_2.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00780a3a-9da7-40a0-9345-9c12d99691a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "static_quantized_model.eval()\n",
    "\n",
    "int8_model_save_path = os.path.join(project_root_dir, 'outputs', 'models', 'int8_model_weights.pth')\n",
    "\n",
    "script_model = torch.jit.script(static_quantized_model)\n",
    "torch.jit.save(script_model, int8_model_save_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58bd6058-17dd-43e8-85d7-35c31190adab",
   "metadata": {},
   "source": [
    "## Total Parameters for 1-Layer LSTM\n",
    "\n",
    "For a single LSTM layer, with the same input and output dimensions (13 MFCC coefficients and 10 output classes), the total number of parameters simplifies to:\n",
    "\n",
    "$$\n",
    "\\text{Total Parameters} = 4h^2 + 70h + 10\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "215e875b-fc5f-4dd6-bd21-809276dccb92",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (spectgramer)",
   "language": "python",
   "name": "innatera-kernel"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
